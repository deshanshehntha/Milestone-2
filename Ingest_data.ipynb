{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dab7f61-ce15-4dbe-8ee2-00c5117de705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (2.3.5)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data/jupyter/venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /data/jupyter/venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /data/jupyter/venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /data/jupyter/venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /data/jupyter/venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /data/jupyter/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/jupyter/venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/jupyter/venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data/jupyter/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /data/jupyter/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/jupyter/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/jupyter/venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /data/jupyter/venv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /data/jupyter/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.12.0━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.12.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.12.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c861456-e7a7-4ed8-82e0-fa5b62c1c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.2 (from langchain)\n",
      "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /data/jupyter/venv/lib/python3.12/site-packages (from langchain) (2.12.5)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.1.2->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.2->langchain)\n",
      "  Downloading langsmith-0.4.59-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.2->langchain)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /data/jupyter/venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_sdk-0.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /data/jupyter/venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /data/jupyter/venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /data/jupyter/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in /data/jupyter/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in /data/jupyter/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /data/jupyter/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /data/jupyter/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /data/jupyter/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /data/jupyter/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /data/jupyter/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /data/jupyter/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /data/jupyter/venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/jupyter/venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.3.0)\n",
      "Downloading langchain-1.1.3-py3-none-any.whl (102 kB)\n",
      "Downloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading langsmith-0.4.59-py3-none-any.whl (413 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, uuid-utils, ormsgpack, jsonpatch, requests-toolbelt, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [langchain]12\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jsonpatch-1.33 langchain-1.1.3 langchain-core-1.2.0 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.0 langsmith-0.4.59 ormsgpack-1.12.1 requests-toolbelt-1.0.0 uuid-utils-0.12.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33569f55-4248-4073-a654-0f9c59c1840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "# from tqdm import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_experimental.text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "076ff3d8-8639-424f-94b1-c4c77aeea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# train_stream = load_dataset(\n",
    "#     \"TechQueen24/DoxplainQA\",\n",
    "#     split=\"train\",\n",
    "#     streaming=True\n",
    "# )\n",
    "\n",
    "# df = pd.DataFrame(list(train_stream))\n",
    "# print(df.shape)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fddd369-5a30-4b38-a56c-174f1fe78e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa = load_dataset(\"hotpotqa/hotpot_qa\", \"fullwiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca4a2e1e-420e-4b4b-a671-b1150103fc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90447"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotqa = hotpotqa[\"train\"].to_pandas()\n",
    "len(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9bb6333-4244-4377-a051-95363e7191cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': array(['Radio City (Indian radio station)',\n",
       "        'History of Albanian football', 'Echosmith',\n",
       "        \"Women's colleges in the Southern United States\",\n",
       "        'First Arthur County Courthouse and Jail', \"Arthur's Magazine\",\n",
       "        '2014–15 Ukrainian Hockey Championship', 'First for Women',\n",
       "        'Freeway Complex Fire', 'William Rast'], dtype=object),\n",
       " 'sentences': array([array([\"Radio City is India's first private FM radio station and was started on 3 July 2001.\",\n",
       "               ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).',\n",
       "               ' It plays Hindi, English and regional songs.',\n",
       "               ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.',\n",
       "               ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.',\n",
       "               ' The Radio station currently plays a mix of Hindi and Regional music.',\n",
       "               ' Abraham Thomas is the CEO of the company.'], dtype=object)                                                                                                                                       ,\n",
       "        array(['Football in Albania existed before the Albanian Football Federation (FSHF) was created.',\n",
       "               \" This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .\",\n",
       "               ' Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.',\n",
       "               ' In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.'],\n",
       "              dtype=object)                                                                                                                                                                                                                                                           ,\n",
       "        array(['Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.',\n",
       "               ' Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.',\n",
       "               ' Echosmith started first as \"Ready Set Go!\"',\n",
       "               ' until they signed to Warner Bros.', ' Records in May 2012.',\n",
       "               ' They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.',\n",
       "               ' The song was Warner Bros.',\n",
       "               \" Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.\",\n",
       "               ' The band\\'s debut album, \"Talking Dreams\", was released on October 8, 2013.'],\n",
       "              dtype=object)                                                                                                                                                                                                                                          ,\n",
       "        array([\"Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.\",\n",
       "               \" Many started first as girls' seminaries or academies.\",\n",
       "               ' Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.',\n",
       "               ' Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.'],\n",
       "              dtype=object)                                                                                                                                                                                                                                                         ,\n",
       "        array(['The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.'],\n",
       "              dtype=object)                                                                                                                           ,\n",
       "        array([\"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.\",\n",
       "               ' Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.',\n",
       "               ' In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\".'],\n",
       "              dtype=object)                                                                                                                ,\n",
       "        array(['The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.',\n",
       "               ' Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.',\n",
       "               ' Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.',\n",
       "               ' The regular season included just 12 rounds, where all the teams went to the semifinals.',\n",
       "               ' In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.'],\n",
       "              dtype=object)                                                                                                                                       ,\n",
       "        array([\"First for Women is a woman's magazine published by Bauer Media Group in the USA.\",\n",
       "               ' The magazine was started in 1989.',\n",
       "               ' It is based in Englewood Cliffs, New Jersey.',\n",
       "               ' In 2011 the circulation of the magazine was 1,310,696 copies.'],\n",
       "              dtype=object)                                                                       ,\n",
       "        array(['The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.',\n",
       "               ' The fire started as two separate fires on November 15, 2008.',\n",
       "               ' The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.',\n",
       "               ' These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.'],\n",
       "              dtype=object)                                                                                                              ,\n",
       "        array(['William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.',\n",
       "               ' It is most known for their premium jeans.',\n",
       "               ' On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.',\n",
       "               ' The label also produces other clothing items such as jackets and tops.',\n",
       "               ' The company started first as a denim line, later evolving into a men’s and women’s clothing line.'],\n",
       "              dtype=object)                                                                                                                              ],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotqa.iloc[0].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e66d6d7-6a28-4976-95e0-a8d12e09ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpotqa.to_csv(\"/data/hotpotqa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16356d20-7f3d-4724-9cc0-f18e68427159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c5a324e-1453-48fd-97a3-65af18d43a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame with 10 rows...\n",
      "Adding 73 chunks to Chroma...\n",
      "  Added 73/73\n",
      "Done. Collection count: 73\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CHROMA_DB_PATH = \"/data/chromadb\"  # Local path that Docker will mount\n",
    "HOTPOT_DATA_PATH = \"/data/hotpotqa.csv\"\n",
    "COLLECTION_NAME = \"hotpotqa\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "CHROMA_DB_PATH = \"/data/chromadb\"\n",
    "HOTPOT_DATA_PATH = \"/data/hotpotqa.csv\"\n",
    "COLLECTION_NAME = \"hotpotqa\"\n",
    "\n",
    "CHAR_CHUNK_SIZE = 2000\n",
    "CHAR_CHUNK_OVERLAP = 200\n",
    "TOKEN_CHUNK_SIZE = 256\n",
    "TOKEN_CHUNK_OVERLAP = 32\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _to_py(obj):\n",
    "    \"\"\"Convert numpy arrays / pandas objects into pure Python lists/strings recursively.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_to_py(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_py(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _safe_eval_numpy_repr(s: str):\n",
    "    \"\"\"\n",
    "    Safely evaluate a string that contains numpy 'array(...)' repr.\n",
    "    Works for your exact data format (array([...], dtype=object)).\n",
    "    \"\"\"\n",
    "    # Quick gate\n",
    "    if \"array(\" not in s:\n",
    "        return None\n",
    "\n",
    "    # Restricted eval environment:\n",
    "    # - no builtins\n",
    "    # - only allow numpy.array, numpy.nan, and Python 'object' for dtype=object\n",
    "    safe_globals = {\n",
    "        \"__builtins__\": {},\n",
    "        \"array\": np.array,\n",
    "        \"nan\": np.nan,\n",
    "        \"object\": object,\n",
    "    }\n",
    "\n",
    "    # Some exports may have weird unicode spaces; normalize lightly\n",
    "    s2 = s.strip()\n",
    "\n",
    "    try:\n",
    "        return eval(s2, safe_globals, {})\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_context(context_obj):\n",
    "    \"\"\"\n",
    "    Convert HotpotQA 'context' to clean text:\n",
    "    - If it's a string with numpy array repr -> eval it safely -> tolist()\n",
    "    - If it's already dict/np arrays -> tolist()\n",
    "    - Join as:  Title: sentence sentence ...\n",
    "    \"\"\"\n",
    "    # Case 1: already dict-like (from a different loader)\n",
    "    if isinstance(context_obj, dict):\n",
    "        ctx = _to_py(context_obj)\n",
    "        return _context_dict_to_text(ctx)\n",
    "\n",
    "    # Case 2: string\n",
    "    if isinstance(context_obj, str):\n",
    "        s = context_obj.strip()\n",
    "\n",
    "        # Try literal_eval first (works if it’s a real dict/lists string)\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            obj = _to_py(obj)\n",
    "            if isinstance(obj, dict):\n",
    "                return _context_dict_to_text(obj)\n",
    "            return str(obj)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Try numpy-repr eval (this is YOUR current format)\n",
    "        obj = _safe_eval_numpy_repr(s)\n",
    "        if isinstance(obj, dict):\n",
    "            obj = _to_py(obj)\n",
    "            return _context_dict_to_text(obj)\n",
    "\n",
    "        # Fallback: return raw string (but at least we won't crash)\n",
    "        return s\n",
    "\n",
    "    # Fallback for unexpected types\n",
    "    return str(context_obj)\n",
    "\n",
    "\n",
    "def _context_dict_to_text(ctx: dict) -> str:\n",
    "    titles = ctx.get(\"title\", []) or []\n",
    "    sentences = ctx.get(\"sentences\", []) or []\n",
    "\n",
    "    # Ensure list-of-lists\n",
    "    titles = list(titles) if not isinstance(titles, list) else titles\n",
    "    sentences = list(sentences) if not isinstance(sentences, list) else sentences\n",
    "\n",
    "    parts = []\n",
    "    for title, sents in zip(titles, sentences):\n",
    "        if isinstance(sents, (list, tuple)):\n",
    "            paragraph = \" \".join(str(x).strip() for x in sents if str(x).strip())\n",
    "        else:\n",
    "            paragraph = str(sents).strip()\n",
    "\n",
    "        t = str(title).strip()\n",
    "        if t and paragraph:\n",
    "            parts.append(f\"{t}: {paragraph}\")\n",
    "        elif paragraph:\n",
    "            parts.append(paragraph)\n",
    "\n",
    "    # This is now CLEAN natural language\n",
    "    return \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "\n",
    "def chunk_context(context_text: str):\n",
    "    if not context_text or not context_text.strip():\n",
    "        return []\n",
    "\n",
    "    char_chunks = character_splitter.split_text(context_text)\n",
    "    token_chunks = []\n",
    "    for c in char_chunks:\n",
    "        token_chunks.extend(token_splitter.split_text(c))\n",
    "\n",
    "    return [t.strip() for t in token_chunks if t and t.strip()]\n",
    "\n",
    "\n",
    "def load_from_dataframe_chunked(df):\n",
    "    print(f\"Processing DataFrame with {len(df)} rows...\")\n",
    "\n",
    "    documents, metadatas, ids = [], [], []\n",
    "\n",
    "    for row_idx, row in df.iterrows():\n",
    "        qid = str(row.get(\"id\", row.get(\"_id\", row_idx)))\n",
    "\n",
    "        context_text = process_context(row[\"context\"])\n",
    "        chunks = chunk_context(context_text)\n",
    "        if not chunks:\n",
    "            continue\n",
    "\n",
    "        base_meta = {\n",
    "            \"question_id\": qid,\n",
    "            \"question\": str(row.get(\"question\", \"\")),\n",
    "            \"answer\": str(row.get(\"answer\", \"\")),\n",
    "            \"type\": str(row.get(\"type\", \"\")),\n",
    "            \"level\": str(row.get(\"level\", \"\")),\n",
    "        }\n",
    "\n",
    "        for ci, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{qid}::c{ci}\"\n",
    "            documents.append(chunk)\n",
    "            metadatas.append({**base_meta, \"chunk_index\": ci})\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        if (row_idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {row_idx + 1}/{len(df)} rows; chunks so far: {len(documents)}\")\n",
    "\n",
    "    return documents, metadatas, ids\n",
    "\n",
    "\n",
    "def create_chromadb(documents, metadatas, ids, db_path):\n",
    "    os.makedirs(db_path, exist_ok=True)\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=db_path,\n",
    "        settings=Settings(anonymized_telemetry=False, allow_reset=True),\n",
    "    )\n",
    "\n",
    "    embedding_fn = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Delete if exists (don’t crash on first run)\n",
    "    try:\n",
    "        chroma_client.delete_collection(COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Adding {len(documents)} chunks to Chroma...\")\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        j = min(i + batch_size, len(documents))\n",
    "        collection.add(documents=documents[i:j], metadatas=metadatas[i:j], ids=ids[i:j])\n",
    "        print(f\"  Added {j}/{len(documents)}\")\n",
    "\n",
    "    print(\"Done. Collection count:\", collection.count())\n",
    "    return collection\n",
    "\n",
    "\n",
    "# Splitters\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=CHAR_CHUNK_SIZE,\n",
    "    chunk_overlap=CHAR_CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=TOKEN_CHUNK_OVERLAP,\n",
    "    tokens_per_chunk=TOKEN_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(HOTPOT_DATA_PATH)\n",
    "    df = df[:10]\n",
    "    documents, metadatas, ids = load_from_dataframe_chunked(df)\n",
    "    create_chromadb(documents, metadatas, ids, CHROMA_DB_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e21b86-c4d0-440f-999e-52b849534e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca5e02ce9604a8d9f19f2ff811f440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6155c03cb308479cbe24b2b57264f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855c8d0b75ba45cf8ba4b9339c554314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d8432404734276b915c9b1d5404580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/23 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23be8ea0f3104659999cbd44bd9ba99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00023.parquet:   0%|          | 0.00/10.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082b6dc258f3400ca34453152970fa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00023.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a9e95107aa4c9ba27ea3b4b9bc048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00011-of-00023.parquet:   0%|          | 0.00/8.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9813fa9edf3b4c05b568470a3d5cbe29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00023.parquet:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e23886c93a645d18922500eabaeb88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00023.parquet:   0%|          | 0.00/9.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ce6a3a6f6e479682e4075d9df25084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00023.parquet:   0%|          | 0.00/9.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33718e2e89954c6f8421620ea526768c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00023.parquet:   0%|          | 0.00/8.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee96a9d9867f488d9ed4fe58c1ed496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00023.parquet:   0%|          | 0.00/9.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d317e6c0b490e866a686987019a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00023.parquet:   0%|          | 0.00/9.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95206d7fa364bdabd9149a63ced9b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00023.parquet:   0%|          | 0.00/43.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6599acd99641c09187865c950f91f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00023.parquet:   0%|          | 0.00/8.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173443dc0016419db811b5c1936248d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00023.parquet:   0%|          | 0.00/12.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031b992956b84b3396dc412ad473b652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00016-of-00023.parquet:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c797573a8f784dd8bd2c87cb0701e640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00017-of-00023.parquet:   0%|          | 0.00/63.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd88ca09caee42bd95da235d14ea75dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00018-of-00023.parquet:   0%|          | 0.00/10.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6edf424869489b9e7564ceaa4f1e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00023.parquet:   0%|          | 0.00/8.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4e979a2703458ba31511b631b8f0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00023.parquet:   0%|          | 0.00/9.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566d87b99b604b31adf2566a9fa7e38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00021-of-00023.parquet:   0%|          | 0.00/53.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd1f9b789fc4dd3971c20eeb9050f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00019-of-00023.parquet:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa82d2c2dcc4cf6b700f84203c5440c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00020-of-00023.parquet:   0%|          | 0.00/10.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd226570e0cd499eb826a2f7b273eaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00022-of-00023.parquet:   0%|          | 0.00/8.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1b426274154ca7b780e50ddf90c90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00023.parquet:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb88ff89463648f6a5c55232e838a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00023.parquet:   0%|          | 0.00/9.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cbca5145b84e9c80e961beff0aee3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00003.parquet:   0%|          | 0.00/9.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f9f21bfd7f4d799708c7f5dd92a277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00001-of-00003.parquet:   0%|          | 0.00/6.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623f1d1ed3184b2dae77e75251eaa267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00002-of-00003.parquet:   0%|          | 0.00/7.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9613d83bd5b4b5e9fa372a9125efd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00007.parquet:   0%|          | 0.00/8.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad6873643c34528ae4bc16e3e49cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00007.parquet:   0%|          | 0.00/10.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bd1f38c21047e4920995eff67bfd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00002-of-00007.parquet:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e309f5cb7fd4e149223eb7fd8f3fabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00003-of-00007.parquet:   0%|          | 0.00/9.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9e6e5857964241a0ba21735d258410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00004-of-00007.parquet:   0%|          | 0.00/122M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a573ac6bb2634603b2575af5eb0d95ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00005-of-00007.parquet:   0%|          | 0.00/10.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e7b009076242ceaf0f622016e7a456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00006-of-00007.parquet:   0%|          | 0.00/8.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d0b68046444275b542276b31484186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10b6f2ed3cb4014baef3768b1f02934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9386eb97296e4236a5b7aa241fbcfa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c008161bfa8486498f32a925a25d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yzhuang/narrative_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecbdf56-4f4b-4acf-94ad-7fcc83561ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32747"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narative_qa = ds[\"train\"].to_pandas()\n",
    "len(narative_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4289b96d-48e7-4d77-8717-2cd44ebf47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "narative_qa.to_csv(\"/data/narative_qa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c963e1-de91-4451-b44a-32e987ffbe91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>Percival Keene\\n\\n At Madeline Hall, an old ma...</td>\n",
       "      <td>Who is Miss Delmer?</td>\n",
       "      <td>[the elderly spinster aunt of the Earl de Vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>Percival Keene\\n\\n At Madeline Hall, an old ma...</td>\n",
       "      <td>Who does Arabella Mason wed?</td>\n",
       "      <td>[Ben Keene, Delmar's valet, Ben Keene]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>Percival Keene\\n\\n At Madeline Hall, an old ma...</td>\n",
       "      <td>How does Percival Keene get his name?</td>\n",
       "      <td>[Percival is Captain Delmar's first name, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>Percival Keene\\n\\n At Madeline Hall, an old ma...</td>\n",
       "      <td>Who is the bully that steals Percival's lunch?</td>\n",
       "      <td>[his teacher, Mr. O'Gallagher, The schoolmaster]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>Percival Keene\\n\\n At Madeline Hall, an old ma...</td>\n",
       "      <td>How does Percival get even with O'Gallagher af...</td>\n",
       "      <td>[He sets them on fire with the teacher sitting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  0029bdbe75423337b551e42bb31f9a102785376f   \n",
       "1  0029bdbe75423337b551e42bb31f9a102785376f   \n",
       "2  0029bdbe75423337b551e42bb31f9a102785376f   \n",
       "3  0029bdbe75423337b551e42bb31f9a102785376f   \n",
       "4  0029bdbe75423337b551e42bb31f9a102785376f   \n",
       "\n",
       "                                             context  \\\n",
       "0  Percival Keene\\n\\n At Madeline Hall, an old ma...   \n",
       "1  Percival Keene\\n\\n At Madeline Hall, an old ma...   \n",
       "2  Percival Keene\\n\\n At Madeline Hall, an old ma...   \n",
       "3  Percival Keene\\n\\n At Madeline Hall, an old ma...   \n",
       "4  Percival Keene\\n\\n At Madeline Hall, an old ma...   \n",
       "\n",
       "                                            question  \\\n",
       "0                                Who is Miss Delmer?   \n",
       "1                       Who does Arabella Mason wed?   \n",
       "2              How does Percival Keene get his name?   \n",
       "3     Who is the bully that steals Percival's lunch?   \n",
       "4  How does Percival get even with O'Gallagher af...   \n",
       "\n",
       "                                              answer  \n",
       "0  [the elderly spinster aunt of the Earl de Vers...  \n",
       "1             [Ben Keene, Delmar's valet, Ben Keene]  \n",
       "2  [Percival is Captain Delmar's first name, and ...  \n",
       "3   [his teacher, Mr. O'Gallagher, The schoolmaster]  \n",
       "4  [He sets them on fire with the teacher sitting...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narative_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3129104-480d-4775-b3c0-eed7079a1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the elderly spinster aunt of the Earl de Verseley and Captain Delmar',\n",
       "       \"She's Captail Delmar's aunt.\"], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narative_qa.iloc[0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad38133-98de-426c-aecd-3ddcbeb0f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NarrativeQA rows: 10 from /data/narative_qa.csv\n",
      "Adding 10160 chunks to Chroma collection 'narrativeqa' ...\n",
      "  Added 1000/10160\n",
      "  Added 2000/10160\n",
      "  Added 3000/10160\n",
      "  Added 4000/10160\n",
      "  Added 5000/10160\n",
      "  Added 6000/10160\n",
      "  Added 7000/10160\n",
      "  Added 8000/10160\n",
      "  Added 9000/10160\n",
      "  Added 10000/10160\n",
      "  Added 10160/10160\n",
      "Done. Collection count: 10160\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CHROMA_DB_PATH = os.getenv(\"CHROMA_DB_PATH\", \"/data/chromadb\")\n",
    "NARRATIVE_CSV_PATH = os.getenv(\"NARRATIVE_CSV_PATH\", \"/data/narative_qa.csv\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"narrativeqa\")\n",
    "\n",
    "CHAR_CHUNK_SIZE = int(os.getenv(\"CHAR_CHUNK_SIZE\", \"2000\"))\n",
    "CHAR_CHUNK_OVERLAP = int(os.getenv(\"CHAR_CHUNK_OVERLAP\", \"200\"))\n",
    "TOKEN_CHUNK_SIZE = int(os.getenv(\"TOKEN_CHUNK_SIZE\", \"256\"))\n",
    "TOKEN_CHUNK_OVERLAP = int(os.getenv(\"TOKEN_CHUNK_OVERLAP\", \"32\"))\n",
    "\n",
    "MAX_ROWS = os.getenv(\"MAX_ROWS\", \"10\")  # \"None\" or integer\n",
    "MAX_ROWS = None if str(MAX_ROWS).lower() == \"none\" else int(MAX_ROWS)\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# -----------------------------\n",
    "# Splitters\n",
    "# -----------------------------\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=CHAR_CHUNK_SIZE,\n",
    "    chunk_overlap=CHAR_CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=TOKEN_CHUNK_OVERLAP,\n",
    "    tokens_per_chunk=TOKEN_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _safe_eval_numpy_array_repr(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    if \"array(\" not in s:\n",
    "        return None\n",
    "\n",
    "    safe_globals = {\n",
    "        \"__builtins__\": {},\n",
    "        \"array\": np.array,\n",
    "        \"nan\": np.nan,\n",
    "        \"object\": object,\n",
    "    }\n",
    "    try:\n",
    "        return eval(s.strip(), safe_globals, {})\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_answer_first(answer_obj) -> str:\n",
    "    answers = []\n",
    "\n",
    "    if isinstance(answer_obj, np.ndarray):\n",
    "        answers = answer_obj.tolist()\n",
    "    elif isinstance(answer_obj, (list, tuple)):\n",
    "        answers = list(answer_obj)\n",
    "    elif isinstance(answer_obj, str):\n",
    "        s = answer_obj.strip()\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            if isinstance(obj, (list, tuple)):\n",
    "                answers = list(obj)\n",
    "            else:\n",
    "                answers = [obj]\n",
    "        except Exception:\n",
    "            arr = _safe_eval_numpy_array_repr(s)\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                answers = arr.tolist()\n",
    "            else:\n",
    "                answers = [s] if s else []\n",
    "    else:\n",
    "        answers = [answer_obj]\n",
    "\n",
    "    answers = [str(a).strip() for a in answers if a is not None and str(a).strip()]\n",
    "    return answers[0] if answers else \"\"\n",
    "\n",
    "def clean_context(context: str) -> str:\n",
    "    return \"\" if context is None else str(context).strip()\n",
    "\n",
    "def chunk_text(text: str):\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    char_chunks = character_splitter.split_text(text)\n",
    "    token_chunks = []\n",
    "    for c in char_chunks:\n",
    "        token_chunks.extend(token_splitter.split_text(c))\n",
    "    return [t.strip() for t in token_chunks if t and t.strip()]\n",
    "\n",
    "# -----------------------------\n",
    "# Build docs/metas/ids\n",
    "# -----------------------------\n",
    "def build_records(df: pd.DataFrame):\n",
    "    documents, metadatas, ids = [], [], []\n",
    "\n",
    "    for row_idx, row in df.iterrows():\n",
    "        story_id = str(row.get(\"id\", row.get(\"_id\", \"unknown\"))).strip()\n",
    "\n",
    "        # ✅ UNIQUE per-question id (story id + row index)\n",
    "        question_id = f\"{story_id}::q{row_idx}\"\n",
    "\n",
    "        question = str(row.get(\"question\", \"\")).strip()\n",
    "        context_text = clean_context(row.get(\"context\", \"\"))\n",
    "\n",
    "        chunks = chunk_text(context_text)\n",
    "        if not chunks:\n",
    "            continue\n",
    "\n",
    "        answer = parse_answer_first(row.get(\"answer\", \"\"))\n",
    "\n",
    "        base_meta = {\n",
    "            \"question_id\": question_id,   # ✅ unique id per question (important!)\n",
    "            \"story_id\": story_id,         # keep original doc/story id\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"dataset\": \"narrativeqa\",\n",
    "        }\n",
    "\n",
    "        for ci, chunk in enumerate(chunks):\n",
    "            # ✅ chunk ids now unique per question (no collisions)\n",
    "            chunk_id = f\"{question_id}::c{ci}\"\n",
    "            documents.append(chunk)\n",
    "            metadatas.append({**base_meta, \"chunk_index\": int(ci)})\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        if (row_idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {row_idx + 1}/{len(df)} rows; chunks so far: {len(documents)}\")\n",
    "\n",
    "    return documents, metadatas, ids\n",
    "\n",
    "# -----------------------------\n",
    "# Write Chroma collection\n",
    "# -----------------------------\n",
    "def write_chroma(documents, metadatas, ids):\n",
    "    os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
    "\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=CHROMA_DB_PATH,\n",
    "        settings=Settings(anonymized_telemetry=False, allow_reset=True),\n",
    "    )\n",
    "\n",
    "    embedding_fn = SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "    try:\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    col = client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Adding {len(documents)} chunks to Chroma collection '{COLLECTION_NAME}' ...\")\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        j = min(i + batch_size, len(documents))\n",
    "        col.add(documents=documents[i:j], metadatas=metadatas[i:j], ids=ids[i:j])\n",
    "        print(f\"  Added {j}/{len(documents)}\")\n",
    "\n",
    "    print(\"Done. Collection count:\", col.count())\n",
    "    return col\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(NARRATIVE_CSV_PATH)\n",
    "\n",
    "    df = df[:10]\n",
    "    if MAX_ROWS is not None:\n",
    "        df = df.iloc[:MAX_ROWS].copy()\n",
    "\n",
    "    print(f\"Loaded NarrativeQA rows: {len(df)} from {NARRATIVE_CSV_PATH}\")\n",
    "    docs, metas, ids = build_records(df)\n",
    "    write_chroma(docs, metas, ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2df60-fc6e-46b9-8398-32e3ae595b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
