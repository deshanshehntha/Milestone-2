version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    volumes:
      - /data/ollama:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  setup-ollama:
    image: ollama/ollama:latest
    container_name: setup-ollama
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    environment:
      OLLAMA_HOST: http://ollama:11434
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    entrypoint: ["/bin/sh", "-lc"]
    command: >
      "ollama pull llama3 &&
       ollama pull qwen2.5 &&
       ollama pull mistral"
    volumes:
      - /data/ollama:/root/.ollama
    restart: "no"

  ollama-llama3:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-llama3
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-llama3
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: llama3
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  ollama-qwen:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-qwen
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-qwen
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: qwen2.5
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  ollama-mistral:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-mistral
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-mistral
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: mistral
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - /data/minio:/data
    restart: unless-stopped

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: ["/bin/sh", "-lc"]
    command: >
      "
      mc alias set local http://minio:9000 minioadmin minioadmin &&
      mc mb -p local/router-artifacts || true &&
      mc anonymous set download local/router-artifacts || true
      "
    restart: "no"


  router:
    build:
      context: ./router
      dockerfile: Dockerfile
    container_name: qa-router
    ports:
      - "50050:50050"
    environment:
      BACKENDS_llama3: "ollama-llama3:50051"
      BACKENDS_qwen2_5: "ollama-qwen:50051"
      BACKENDS_mistral: "ollama-mistral:50051"
      DEFAULT_VARIANT: "llama3"
      TOP_K: "6"

      ROUTER_MODEL_PATH: "/data/router_model/router_model.joblib"
      ROUTER_MODEL_RELOAD_S: "30"

      ROUTER_MODEL_S3_FETCH: "1"
      S3_ENDPOINT_URL: "http://minio:9000"
      S3_ACCESS_KEY: "minioadmin"
      S3_SECRET_KEY: "minioadmin"
      S3_BUCKET: "router-artifacts"
      S3_KEY: "router/router_model.joblib"

    volumes:
      - /data/router_model:/data/router_model

    depends_on:
      - minio-init
      - ollama-llama3
      - ollama-qwen
      - ollama-mistral
    restart: unless-stopped


  evaluate-pipeline:
    build:
      context: .
      dockerfile: evaluate/Dockerfile
    container_name: evaluate-pipeline
    depends_on:
      - router
    environment:
      ROUTER_ADDR: qa-router:50050
      MODELS: "llama3,qwen2_5,mistral"
      GRPC_TIMEOUT_S: "180"
      MAX_SAMPLES: "100"
      TOP_K: "8"
      JSON_LOGS: "1"
      TRAIN_ROUTER_MODEL: "0"
    volumes:
      - /data/results:/data/results
      - /data/chromadb:/chromadb
      - /data/hotpotqa.csv:/data/hotpotqa.csv
    restart: "no"


  # ML router trainer: outputs artifact with {router_type:"ml", ml_model:...}
  router-trainer-ml:
    build:
      context: .
      dockerfile: evaluate/Dockerfile.trainer_ml
    container_name: router-trainer-ml
    depends_on:
      - minio-init
    environment:
      ROUTER_MODEL_PATH: "/data/router_model/router_model.joblib"
      ROUTER_TRAIN_GLOB: "/data/results/**/evaluation_results.csv"
      ROUTER_EM_WEIGHT: "0.25"
      ROUTER_LAT_WEIGHT: "0.0002"
      S3_ENDPOINT_URL: "http://minio:9000"
      S3_ACCESS_KEY: "minioadmin"
      S3_SECRET_KEY: "minioadmin"
      S3_BUCKET: "router-artifacts"
      S3_KEY: "router/router_model.joblib"
      S3_REGION: "us-east-1"
    volumes:
      - /data/results:/data/results
      - /data/router_model:/data/router_model
    command: ["python", "-u", "evaluate/train_ml_router.py"]
    restart: "no"

  # Dataset router trainer: outputs artifact with {router_type:"dataset", dataset_router:{...}}
  router-trainer-dataset:
    build:
      context: .
      dockerfile: evaluate/Dockerfile.trainer_dataset
    container_name: router-trainer-dataset
    depends_on:
      - minio-init
    environment:
      ROUTER_MODEL_PATH: "/data/router_model/router_model.joblib"
      ROUTER_TRAIN_GLOB: "/data/results/**/evaluation_results.csv"
      ROUTER_EM_WEIGHT: "0.25"
      ROUTER_LAT_WEIGHT: "0.0002"
      S3_ENDPOINT_URL: "http://minio:9000"
      S3_ACCESS_KEY: "minioadmin"
      S3_SECRET_KEY: "minioadmin"
      S3_BUCKET: "router-artifacts"
      S3_KEY: "router/router_model.joblib"
      S3_REGION: "us-east-1"
    volumes:
      - /data/results:/data/results
      - /data/router_model:/data/router_model
    command: ["python", "-u", "evaluate/train_dataset_router.py"]
    restart: "no"

  evaluate-quick:
    build:
      context: .
      dockerfile: evaluate/Dockerfile.inference
    container_name: evaluate-quick
    environment:
      PYTHONUNBUFFERED: "1"
      ROUTER_ADDR: qa-router:50050
      CHROMA_DB_PATH: /chromadb
      MAX_SAMPLES: "1000"
      TOP_K: "8"
      GRPC_TIMEOUT_S: "60"
      RESULTS_PATH: "/data/results_quick"
      RUN_ID: "latest"  
      EVAL_COLLECTIONS: "hotpotqa,narrativeqa,pubmedqa"
      JSON_LOGS: "1"
      RESTRICT_TO_SAME_QUESTION: "1"
      EMBED_MODEL: "all-MiniLM-L6-v2"
      # IMPORTANT for “router switching” tests:
      # set MODELS=auto and in evaluate_quick.py handle it by NOT sending ("variant", ...)
      MODELS: "auto"
    volumes:
      - /data/chromadb:/chromadb
      - /data/results_quick:/data/results_quick
    restart: "no"

  eval-ui:
    build:
      context: ./eval_ui
      dockerfile: Dockerfile
    container_name: eval-ui
    ports:
      - "8081:8501"
    environment:
      PYTHONUNBUFFERED: "1"
      ROUTER_ADDR: qa-router:50050
      CHROMA_DB_PATH: /chromadb
      RESULTS_PATH: "/data/results_quick"
      EVAL_COLLECTIONS: "hotpotqa,narrativeqa,pubmedqa"
      PER_COLLECTION_SAMPLES: "300"
      PEEK_LIMIT: "5000"
      TOP_K: "8"
      GRPC_TIMEOUT_S: "60"
      JSON_LOGS: "1"
      RESTRICT_TO_SAME_QUESTION: "1"
      EMBED_MODEL: "all-MiniLM-L6-v2"
      MIX_SEED: "42"
    volumes:
      - /data/chromadb:/chromadb
      - /data/results_quick:/data/results_quic
    depends_on:
      - router
    restart: unless-stopped
