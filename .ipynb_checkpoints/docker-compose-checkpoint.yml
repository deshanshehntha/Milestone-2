version: "3.9"

services:
  # -------------------------
  # Ollama (persist models on /data)
  # -------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: ollama

    # ✅ GPU access (Compose v3.9-compatible)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]

    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optional:
      # OLLAMA_CONTEXT_LENGTH: "4096"

    volumes:
      - /data/ollama:/root/.ollama

    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30

    restart: unless-stopped

  # Pull models once (so servers don't race on first run)
  setup-ollama:
    image: ollama/ollama:latest
    container_name: setup-ollama
    depends_on:
      ollama:
        condition: service_healthy

    # ✅ GPU access (not required for pulling, but harmless)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]

    environment:
      OLLAMA_HOST: http://ollama:11434
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility

    entrypoint: ["/bin/sh", "-lc"]
    command: >
      "ollama pull llama3 &&
       ollama pull qwen2.5 &&
       ollama pull mistral"

    volumes:
      - /data/ollama:/root/.ollama

    restart: "no"

  # -------------------------
  # Ollama-backed gRPC model servers
  # -------------------------
  ollama-llama3:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-llama3
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-llama3
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: llama3
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  ollama-qwen:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-qwen
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-qwen
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: qwen2.5
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  ollama-mistral:
    build:
      context: ./models/server
      dockerfile: Dockerfile
    container_name: ollama-mistral
    expose:
      - "50051"
    environment:
      MODEL_NAME: ollama-mistral
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: mistral
      MAX_NEW_TOKENS: "128"
      OLLAMA_TIMEOUT_S: "180"
    depends_on:
      setup-ollama:
        condition: service_completed_successfully
    restart: unless-stopped

  # -------------------------
  # Router (Ollama-only)
  # -------------------------
  router:
    build:
      context: ./router
      dockerfile: Dockerfile
    container_name: qa-router
    ports:
      - "50050:50050"
    environment:
      BACKENDS_llama3: "ollama-llama3:50051"
      BACKENDS_qwen2_5: "ollama-qwen:50051"
      BACKENDS_mistral: "ollama-mistral:50051"
      DEFAULT_VARIANT: "llama3"
    depends_on:
      - ollama-llama3
      - ollama-qwen
      - ollama-mistral
    restart: unless-stopped

  # -------------------------
  # Evaluation pipeline
  # -------------------------
  evaluate-pipeline:
    build:
      context: .
      dockerfile: evaluate/Dockerfile
    container_name: evaluate-pipeline
    depends_on:
      - router
    environment:
      ROUTER_ADDR: qa-router:50050
      MODELS: "llama3,qwen2_5,mistral"
      GRPC_TIMEOUT_S: "180"
      MAX_SAMPLES: "10"
    volumes:
      - /data/results:/data/results
      - /data/chromadb:/chromadb
      - /data/hotpotqa.csv:/data/hotpotqa.csv
