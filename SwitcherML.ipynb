{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "977350fe-3476-42a2-9eb7-bb7196bc39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /data/jupyter/venv/lib/python3.12/site-packages (from lightgbm) (2.3.5)\n",
      "Requirement already satisfied: scipy in /data/jupyter/venv/lib/python3.12/site-packages (from lightgbm) (1.16.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7582da1f-4bd8-4ce5-b8c6-66568e06caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f87d2d9-b444-45c4-a07c-c4cd0b7a44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_model(group: pd.DataFrame) -> str:\n",
    "    em_w = float(os.getenv(\"ROUTER_EM_WEIGHT\", \"0.25\"))\n",
    "    lat_w = float(os.getenv(\"ROUTER_LAT_WEIGHT\", \"0.0002\"))\n",
    "\n",
    "    inf_ms = group.get(\"inference_ms\", pd.Series([0.0] * len(group))).fillna(\n",
    "        group.get(\"client_elapsed_ms\", pd.Series([0.0] * len(group))).fillna(0.0)\n",
    "    )\n",
    "    utility = (\n",
    "        group.get(\"answer_cosine_sim\", 0.0).fillna(0.0)\n",
    "        + em_w * group.get(\"exact_match\", 0.0).fillna(0.0)\n",
    "        - lat_w * inf_ms\n",
    "    )\n",
    "\n",
    "    best_idx = utility.idxmax()\n",
    "    return str(group.loc[best_idx, \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9804540-f654-4e27-8baf-d01edd63576a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['collection', 'dataset', 'story_id', 'question_id', 'question',\n",
       "       'true_answer', 'predicted_answer', 'model', 'confidence',\n",
       "       'answer_cosine_sim', 'exact_match', 'type', 'level', 'retrieval_k',\n",
       "       'router_retrieval_ms', 'inference_ms', 'end_to_end_ms',\n",
       "       'client_elapsed_ms', 'local_retrieval_ms', 'retr_cache_hit',\n",
       "       'question_hash', 'prompt_chars', 'context_chars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "out_path = os.getenv(\"ROUTER_MODEL_PATH\", \"/data/router_model/router_model.joblib\")\n",
    "\n",
    "csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "if not csvs:\n",
    "    raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e76bf1ce-35c7-46b4-bf32-8315f165cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    out_path = os.getenv(\"ROUTER_MODEL_PATH\", \"/data/router_model/router_model.joblib\")\n",
    "\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    # --- normalize columns / make dataset always present ---\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    # fallback: dataset := collection if empty\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    # ensure feature columns exist\n",
    "    for c in [\"prompt_chars\", \"context_chars\", \"retrieval_k\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "\n",
    "    # must have these to train\n",
    "    required = [\"question_id\", \"model\", \"answer_cosine_sim\", \"exact_match\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise SystemExit(f\"Missing required columns in eval CSV(s): {missing}\")\n",
    "\n",
    "    # group key (prefer question_id)\n",
    "    key = \"question_id\"\n",
    "\n",
    "    # labels: best model per question\n",
    "    y = df.groupby(key).apply(pick_best_model)\n",
    "\n",
    "    # features: first row per question\n",
    "    first = df.groupby(key).first().copy()\n",
    "    X = first[[\"dataset\", \"prompt_chars\", \"context_chars\", \"retrieval_k\"]].copy()\n",
    "\n",
    "    # build model\n",
    "    cat = [\"dataset\"]\n",
    "    num = [\"prompt_chars\", \"context_chars\", \"retrieval_k\"]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat),\n",
    "            (\"num\", \"passthrough\", num),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    # os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    # joblib.dump({\"model\": pipe}, out_path)\n",
    "    # print(f\"Saved router model: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ea3ce7-862d-4fa2-8202-2d058b37f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/1182117579.py:39: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y = df.groupby(key).apply(pick_best_model)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62cdc309-285e-44cb-8dff-16e73f100a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "if not csvs:\n",
    "    raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aa9e00f-2d32-403f-a447-e44ed4b88a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['collection', 'dataset', 'story_id', 'question_id', 'question',\n",
       "       'true_answer', 'predicted_answer', 'model', 'confidence',\n",
       "       'answer_cosine_sim', 'exact_match', 'type', 'level', 'retrieval_k',\n",
       "       'router_retrieval_ms', 'inference_ms', 'end_to_end_ms',\n",
       "       'client_elapsed_ms', 'local_retrieval_ms', 'retr_cache_hit',\n",
       "       'question_hash', 'prompt_chars', 'context_chars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "563e73b4-fab0-4e53-92ba-9097bc711d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 600\n",
      "unique questions: 200\n",
      "models per question (approx): 3.0\n",
      "unique models: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"rows:\", len(df))\n",
    "print(\"unique questions:\", df[\"question_id\"].nunique())\n",
    "print(\"models per question (approx):\", len(df) / df[\"question_id\"].nunique())\n",
    "print(\"unique models:\", df[\"model\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93260973-8504-4c30-a140-9bf6b9e566db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection</th>\n",
       "      <th>dataset</th>\n",
       "      <th>story_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>model</th>\n",
       "      <th>confidence</th>\n",
       "      <th>answer_cosine_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>retrieval_k</th>\n",
       "      <th>router_retrieval_ms</th>\n",
       "      <th>inference_ms</th>\n",
       "      <th>end_to_end_ms</th>\n",
       "      <th>client_elapsed_ms</th>\n",
       "      <th>local_retrieval_ms</th>\n",
       "      <th>retr_cache_hit</th>\n",
       "      <th>question_hash</th>\n",
       "      <th>prompt_chars</th>\n",
       "      <th>context_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a7a06935542990198eaf050</td>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>llama3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>302.126472</td>\n",
       "      <td>302.142379</td>\n",
       "      <td>314.586639</td>\n",
       "      <td>136.697769</td>\n",
       "      <td>False</td>\n",
       "      <td>6e09757d748d</td>\n",
       "      <td>70</td>\n",
       "      <td>5236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a7a06935542990198eaf050</td>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>qwen2_5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>316.251901</td>\n",
       "      <td>316.266435</td>\n",
       "      <td>324.970722</td>\n",
       "      <td>136.697769</td>\n",
       "      <td>False</td>\n",
       "      <td>6e09757d748d</td>\n",
       "      <td>70</td>\n",
       "      <td>5236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a7a06935542990198eaf050</td>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>mistral</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>270.321176</td>\n",
       "      <td>270.335732</td>\n",
       "      <td>279.697180</td>\n",
       "      <td>136.697769</td>\n",
       "      <td>False</td>\n",
       "      <td>6e09757d748d</td>\n",
       "      <td>70</td>\n",
       "      <td>5236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a879ab05542996e4f30887e</td>\n",
       "      <td>The Oberoi family is part of a hotel company t...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>llama3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>310.539260</td>\n",
       "      <td>310.556257</td>\n",
       "      <td>321.772575</td>\n",
       "      <td>61.217308</td>\n",
       "      <td>False</td>\n",
       "      <td>a2d6cf3d26a3</td>\n",
       "      <td>81</td>\n",
       "      <td>5816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a879ab05542996e4f30887e</td>\n",
       "      <td>The Oberoi family is part of a hotel company t...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>qwen2_5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>332.341728</td>\n",
       "      <td>332.357168</td>\n",
       "      <td>342.782974</td>\n",
       "      <td>61.217308</td>\n",
       "      <td>False</td>\n",
       "      <td>a2d6cf3d26a3</td>\n",
       "      <td>81</td>\n",
       "      <td>5816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd::q98</td>\n",
       "      <td>What city has Olivia fled from?</td>\n",
       "      <td>Olivia has fled from Akif.Akif</td>\n",
       "      <td>unknown</td>\n",
       "      <td>qwen2_5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>386.604054</td>\n",
       "      <td>386.617187</td>\n",
       "      <td>397.042990</td>\n",
       "      <td>480.764151</td>\n",
       "      <td>False</td>\n",
       "      <td>f244d0d26d70</td>\n",
       "      <td>31</td>\n",
       "      <td>8051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd::q98</td>\n",
       "      <td>What city has Olivia fled from?</td>\n",
       "      <td>Olivia has fled from Akif.Akif</td>\n",
       "      <td>Unknown (The context does not provide informat...</td>\n",
       "      <td>mistral</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.666956</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>431.908602</td>\n",
       "      <td>431.921200</td>\n",
       "      <td>442.307711</td>\n",
       "      <td>480.764151</td>\n",
       "      <td>False</td>\n",
       "      <td>f244d0d26d70</td>\n",
       "      <td>31</td>\n",
       "      <td>8051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99</td>\n",
       "      <td>Who is chasing and pursuing olivia?</td>\n",
       "      <td>Shah Amurath is chasing Olivia.Shah</td>\n",
       "      <td>unknown</td>\n",
       "      <td>llama3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.203123</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>368.137840</td>\n",
       "      <td>368.151485</td>\n",
       "      <td>379.623175</td>\n",
       "      <td>465.050220</td>\n",
       "      <td>False</td>\n",
       "      <td>5b2f39476ebd</td>\n",
       "      <td>35</td>\n",
       "      <td>8052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99</td>\n",
       "      <td>Who is chasing and pursuing olivia?</td>\n",
       "      <td>Shah Amurath is chasing Olivia.Shah</td>\n",
       "      <td>unknown</td>\n",
       "      <td>qwen2_5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.203123</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>378.992995</td>\n",
       "      <td>379.005448</td>\n",
       "      <td>389.173746</td>\n",
       "      <td>465.050220</td>\n",
       "      <td>False</td>\n",
       "      <td>5b2f39476ebd</td>\n",
       "      <td>35</td>\n",
       "      <td>8052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99</td>\n",
       "      <td>Who is chasing and pursuing olivia?</td>\n",
       "      <td>Shah Amurath is chasing Olivia.Shah</td>\n",
       "      <td>unknown (The context does not provide informat...</td>\n",
       "      <td>mistral</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.423670</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>420.961641</td>\n",
       "      <td>420.977110</td>\n",
       "      <td>430.461884</td>\n",
       "      <td>465.050220</td>\n",
       "      <td>False</td>\n",
       "      <td>5b2f39476ebd</td>\n",
       "      <td>35</td>\n",
       "      <td>8052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      collection      dataset                                  story_id  \\\n",
       "0       hotpotqa     hotpotqa                                       NaN   \n",
       "1       hotpotqa     hotpotqa                                       NaN   \n",
       "2       hotpotqa     hotpotqa                                       NaN   \n",
       "3       hotpotqa     hotpotqa                                       NaN   \n",
       "4       hotpotqa     hotpotqa                                       NaN   \n",
       "..           ...          ...                                       ...   \n",
       "595  narrativeqa  narrativeqa  00950a3641e6a28b04a6fabf6334140e2deaa9fd   \n",
       "596  narrativeqa  narrativeqa  00950a3641e6a28b04a6fabf6334140e2deaa9fd   \n",
       "597  narrativeqa  narrativeqa  00950a3641e6a28b04a6fabf6334140e2deaa9fd   \n",
       "598  narrativeqa  narrativeqa  00950a3641e6a28b04a6fabf6334140e2deaa9fd   \n",
       "599  narrativeqa  narrativeqa  00950a3641e6a28b04a6fabf6334140e2deaa9fd   \n",
       "\n",
       "                                       question_id  \\\n",
       "0                         5a7a06935542990198eaf050   \n",
       "1                         5a7a06935542990198eaf050   \n",
       "2                         5a7a06935542990198eaf050   \n",
       "3                         5a879ab05542996e4f30887e   \n",
       "4                         5a879ab05542996e4f30887e   \n",
       "..                                             ...   \n",
       "595  00950a3641e6a28b04a6fabf6334140e2deaa9fd::q98   \n",
       "596  00950a3641e6a28b04a6fabf6334140e2deaa9fd::q98   \n",
       "597  00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99   \n",
       "598  00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99   \n",
       "599  00950a3641e6a28b04a6fabf6334140e2deaa9fd::q99   \n",
       "\n",
       "                                              question  \\\n",
       "0    Which magazine was started first Arthur's Maga...   \n",
       "1    Which magazine was started first Arthur's Maga...   \n",
       "2    Which magazine was started first Arthur's Maga...   \n",
       "3    The Oberoi family is part of a hotel company t...   \n",
       "4    The Oberoi family is part of a hotel company t...   \n",
       "..                                                 ...   \n",
       "595                    What city has Olivia fled from?   \n",
       "596                    What city has Olivia fled from?   \n",
       "597                Who is chasing and pursuing olivia?   \n",
       "598                Who is chasing and pursuing olivia?   \n",
       "599                Who is chasing and pursuing olivia?   \n",
       "\n",
       "                             true_answer  \\\n",
       "0                      Arthur's Magazine   \n",
       "1                      Arthur's Magazine   \n",
       "2                      Arthur's Magazine   \n",
       "3                                  Delhi   \n",
       "4                                  Delhi   \n",
       "..                                   ...   \n",
       "595       Olivia has fled from Akif.Akif   \n",
       "596       Olivia has fled from Akif.Akif   \n",
       "597  Shah Amurath is chasing Olivia.Shah   \n",
       "598  Shah Amurath is chasing Olivia.Shah   \n",
       "599  Shah Amurath is chasing Olivia.Shah   \n",
       "\n",
       "                                      predicted_answer    model  confidence  \\\n",
       "0                                    Arthur's Magazine   llama3        0.85   \n",
       "1                                    Arthur's Magazine  qwen2_5        0.85   \n",
       "2                                    Arthur's Magazine  mistral        0.85   \n",
       "3                                                Delhi   llama3        0.85   \n",
       "4                                                Delhi  qwen2_5        0.85   \n",
       "..                                                 ...      ...         ...   \n",
       "595                                            unknown  qwen2_5        0.15   \n",
       "596  Unknown (The context does not provide informat...  mistral        0.85   \n",
       "597                                            unknown   llama3        0.15   \n",
       "598                                            unknown  qwen2_5        0.15   \n",
       "599  unknown (The context does not provide informat...  mistral        0.85   \n",
       "\n",
       "     answer_cosine_sim  ...  retrieval_k router_retrieval_ms inference_ms  \\\n",
       "0             1.000000  ...            6            0.005002   302.126472   \n",
       "1             1.000000  ...            6            0.005136   316.251901   \n",
       "2             1.000000  ...            6            0.004517   270.321176   \n",
       "3             1.000000  ...            6            0.005190   310.539260   \n",
       "4             1.000000  ...            6            0.005054   332.341728   \n",
       "..                 ...  ...          ...                 ...          ...   \n",
       "595           0.186501  ...            6            0.005313   386.604054   \n",
       "596           0.666956  ...            6            0.006018   431.908602   \n",
       "597           0.203123  ...            6            0.006203   368.137840   \n",
       "598           0.203123  ...            6            0.005640   378.992995   \n",
       "599           0.423670  ...            6            0.006288   420.961641   \n",
       "\n",
       "     end_to_end_ms  client_elapsed_ms  local_retrieval_ms  retr_cache_hit  \\\n",
       "0       302.142379         314.586639          136.697769           False   \n",
       "1       316.266435         324.970722          136.697769           False   \n",
       "2       270.335732         279.697180          136.697769           False   \n",
       "3       310.556257         321.772575           61.217308           False   \n",
       "4       332.357168         342.782974           61.217308           False   \n",
       "..             ...                ...                 ...             ...   \n",
       "595     386.617187         397.042990          480.764151           False   \n",
       "596     431.921200         442.307711          480.764151           False   \n",
       "597     368.151485         379.623175          465.050220           False   \n",
       "598     379.005448         389.173746          465.050220           False   \n",
       "599     420.977110         430.461884          465.050220           False   \n",
       "\n",
       "     question_hash  prompt_chars  context_chars  \n",
       "0     6e09757d748d            70           5236  \n",
       "1     6e09757d748d            70           5236  \n",
       "2     6e09757d748d            70           5236  \n",
       "3     a2d6cf3d26a3            81           5816  \n",
       "4     a2d6cf3d26a3            81           5816  \n",
       "..             ...           ...            ...  \n",
       "595   f244d0d26d70            31           8051  \n",
       "596   f244d0d26d70            31           8051  \n",
       "597   5b2f39476ebd            35           8052  \n",
       "598   5b2f39476ebd            35           8052  \n",
       "599   5b2f39476ebd            35           8052  \n",
       "\n",
       "[600 rows x 23 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, dataset, model, confidence, answer_cosine_sim, exact_match, router_retrieval_ms, inference_ms, prompt_chars, context_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e02229c9-13cb-4d12-ac24-6cc38d7284d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 45000\n",
      "unique questions: 15000\n",
      "datasets: 3\n",
      "models: ['llama3', 'mistral', 'qwen2_5']\n",
      "\n",
      "Default (global best by mean EM): qwen2_5\n",
      "\n",
      "=== Best model per dataset ===\n",
      "    dataset best_model  mean_exact_match  mean_cosine\n",
      "   hotpotqa    qwen2_5             0.464     0.691563\n",
      "narrativeqa    mistral             0.000     0.336478\n",
      "   pubmedqa    mistral             0.000     0.238067\n",
      "\n",
      "=== Full leaderboard (top 3 per dataset) ===\n",
      "    dataset   model  mean_exact_match  mean_cosine\n",
      "   hotpotqa qwen2_5            0.4640     0.691563\n",
      "   hotpotqa  llama3            0.4504     0.697455\n",
      "   hotpotqa mistral            0.4348     0.703102\n",
      "narrativeqa  llama3            0.0000     0.283110\n",
      "narrativeqa mistral            0.0000     0.336478\n",
      "narrativeqa qwen2_5            0.0000     0.296356\n",
      "   pubmedqa  llama3            0.0000     0.010169\n",
      "   pubmedqa mistral            0.0000     0.238067\n",
      "   pubmedqa qwen2_5            0.0000     0.031367\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data loading / cleaning\n",
    "# -----------------------------\n",
    "def load_and_normalize(results_glob: str) -> pd.DataFrame:\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    for c in [\"type\", \"level\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"unknown\"\n",
    "        df[c] = df[c].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    # required metrics\n",
    "    if \"exact_match\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: exact_match\")\n",
    "    df[\"exact_match\"] = pd.to_numeric(df[\"exact_match\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        df[\"answer_cosine_sim\"] = pd.to_numeric(df[\"answer_cosine_sim\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    if \"model\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: model\")\n",
    "    df[\"model\"] = df[\"model\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    if \"question_id\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: question_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Best model per dataset\n",
    "# -----------------------------\n",
    "def dataset_leaderboard(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      winners_df: one row per dataset with best model\n",
    "      leaderboard_df: all dataset x model rows with metrics\n",
    "    \"\"\"\n",
    "    agg = {\"exact_match\": \"mean\"}\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        agg[\"answer_cosine_sim\"] = \"mean\"\n",
    "\n",
    "    leaderboard = (\n",
    "        df.groupby([\"dataset\", \"model\"], as_index=False)\n",
    "          .agg(agg)\n",
    "          .rename(columns={\n",
    "              \"exact_match\": \"mean_exact_match\",\n",
    "              \"answer_cosine_sim\": \"mean_cosine\" if \"answer_cosine_sim\" in df.columns else \"mean_cosine\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    sort_cols = [\"dataset\", \"mean_exact_match\"]\n",
    "    ascending = [True, False]\n",
    "    if \"mean_cosine\" in leaderboard.columns:\n",
    "        sort_cols.append(\"mean_cosine\")\n",
    "        ascending.append(False)\n",
    "    sort_cols.append(\"model\")\n",
    "    ascending.append(True)\n",
    "\n",
    "    winners = (\n",
    "        leaderboard.sort_values(sort_cols, ascending=ascending)\n",
    "                   .groupby(\"dataset\", as_index=False)\n",
    "                   .first()\n",
    "                   .rename(columns={\"model\": \"best_model\"})\n",
    "    )\n",
    "\n",
    "    return winners, leaderboard\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Optional: Best per dataset+type+level\n",
    "# -----------------------------\n",
    "def bucket_leaderboard(df: pd.DataFrame, bucket_cols=(\"dataset\", \"type\", \"level\")):\n",
    "    agg = {\"exact_match\": \"mean\"}\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        agg[\"answer_cosine_sim\"] = \"mean\"\n",
    "\n",
    "    leaderboard = (\n",
    "        df.groupby(list(bucket_cols) + [\"model\"], as_index=False)\n",
    "          .agg(agg)\n",
    "          .rename(columns={\n",
    "              \"exact_match\": \"mean_exact_match\",\n",
    "              \"answer_cosine_sim\": \"mean_cosine\" if \"answer_cosine_sim\" in df.columns else \"mean_cosine\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    sort_cols = list(bucket_cols) + [\"mean_exact_match\"]\n",
    "    ascending = [True] * len(bucket_cols) + [False]\n",
    "    if \"mean_cosine\" in leaderboard.columns:\n",
    "        sort_cols.append(\"mean_cosine\")\n",
    "        ascending.append(False)\n",
    "    sort_cols.append(\"model\")\n",
    "    ascending.append(True)\n",
    "\n",
    "    winners = (\n",
    "        leaderboard.sort_values(sort_cols, ascending=ascending)\n",
    "                   .groupby(list(bucket_cols), as_index=False)\n",
    "                   .first()\n",
    "                   .rename(columns={\"model\": \"best_model\"})\n",
    "    )\n",
    "\n",
    "    return winners, leaderboard\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Switcher functions\n",
    "# -----------------------------\n",
    "def make_dataset_switcher(winners_df: pd.DataFrame, default_model: str):\n",
    "    mapping = dict(zip(winners_df[\"dataset\"].tolist(), winners_df[\"best_model\"].tolist()))\n",
    "\n",
    "    def choose(dataset: str) -> str:\n",
    "        d = (dataset or \"unknown\").strip().lower()\n",
    "        return mapping.get(d, default_model)\n",
    "\n",
    "    return choose, mapping\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    df = load_and_normalize(results_glob)\n",
    "\n",
    "    print(\"rows:\", len(df))\n",
    "    print(\"unique questions:\", df[\"question_id\"].nunique())\n",
    "    print(\"datasets:\", df[\"dataset\"].nunique())\n",
    "    print(\"models:\", sorted(df[\"model\"].unique()))\n",
    "\n",
    "    # choose a sane default model = best overall mean exact_match\n",
    "    default_model = (\n",
    "        df.groupby(\"model\")[\"exact_match\"].mean().sort_values(ascending=False).index[0]\n",
    "    )\n",
    "    print(\"\\nDefault (global best by mean EM):\", default_model)\n",
    "\n",
    "    # -------- dataset-only winners --------\n",
    "    winners, leaderboard = dataset_leaderboard(df)\n",
    "\n",
    "    print(\"\\n=== Best model per dataset ===\")\n",
    "    print(winners.sort_values(\"dataset\").to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== Full leaderboard (top 3 per dataset) ===\")\n",
    "    top3 = leaderboard.sort_values([\"dataset\", \"mean_exact_match\"], ascending=[True, False]) \\\n",
    "                      .groupby(\"dataset\").head(3)\n",
    "    print(top3.to_string(index=False))\n",
    "\n",
    "    choose_model, mapping = make_dataset_switcher(winners, default_model)\n",
    "\n",
    "    # Example usage:\n",
    "    # print(\"\\nExample:\", \"hotpotqa ->\", choose_model(\"hotpotqa\"))\n",
    "\n",
    "    # -------- optional: dataset+type+level winners --------\n",
    "    # bucket_winners, bucket_lb = bucket_leaderboard(df, (\"dataset\",\"type\",\"level\"))\n",
    "    # print(\"\\n=== Best model per dataset+type+level ===\")\n",
    "    # print(bucket_winners.head(50).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22f223d7-643d-46c4-937e-bccf254ae9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract these features from your data:\n",
    "features = [\n",
    "    # Existing\n",
    "    'dataset',  # one-hot encoded\n",
    "    'prompt_chars',\n",
    "    'context_chars',\n",
    "    \n",
    "    # Derive new ones\n",
    "    'question_length',  # chars in question\n",
    "    'context_to_prompt_ratio',  # context_chars / prompt_chars\n",
    "    'has_numbers',  # boolean: question contains digits\n",
    "    'question_marks_count',  # complexity indicator\n",
    "    'avg_word_length',  # vocabulary complexity\n",
    "]\n",
    "\n",
    "# Target: best_model for each question (based on which got highest EM or cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d4847db-acd4-4dc0-8557-ceb1ffe4113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total rows: 45000\n",
      "Unique questions: 15000\n",
      "Datasets: 3 - ['hotpotqa', 'narrativeqa', 'pubmedqa']\n",
      "Models: ['llama3', 'mistral', 'qwen2_5']\n",
      "\n",
      "=== Training ML Router ===\n",
      "\n",
      "=== Preparing training data (optimizing for exact_match) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/2940808754.py:118: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  best_models = df.groupby(\"question_id\").apply(best_model_for_question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 15000\n",
      "Target distribution:\n",
      "target_model\n",
      "llama3     7729\n",
      "mistral    5077\n",
      "qwen2_5    2194\n",
      "Name: count, dtype: int64\n",
      "Train size: 12000, Test size: 3000\n",
      "\n",
      "Test Accuracy: 0.5333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      llama3       0.56      0.73      0.64      1546\n",
      "     mistral       0.47      0.46      0.47      1015\n",
      "     qwen2_5       0.40      0.00      0.01       439\n",
      "\n",
      "    accuracy                           0.53      3000\n",
      "   macro avg       0.48      0.40      0.37      3000\n",
      "weighted avg       0.51      0.53      0.49      3000\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                feature  importance\n",
      "          context_chars         807\n",
      "context_to_prompt_ratio         792\n",
      "        avg_word_length         652\n",
      "           prompt_chars         597\n",
      "    question_word_count         270\n",
      "        question_length         109\n",
      "       dataset_hotpotqa          70\n",
      "            type_bridge          49\n",
      "            has_numbers          48\n",
      "             level_easy          36\n",
      "Router saved to ml_router.pkl\n",
      "\n",
      "=== Evaluating Router Performance ===\n",
      "\n",
      "Strategy Comparison:\n",
      "                mean_exact_match  mean_cosine\n",
      "ML Router               0.150600     0.399978\n",
      "Always Qwen2.5          0.154667     0.339762\n",
      "Dataset Best            0.154667     0.328281\n",
      "\n",
      "============================================================\n",
      "EXAMPLE INFERENCE\n",
      "============================================================\n",
      "\n",
      "Question: Who was the first president of the United States?\n",
      "Dataset: hotpotqa\n",
      "Predicted Model: llama3\n",
      "\n",
      "============================================================\n",
      "To load router later:\n",
      "  router = MLRouter.load('ml_router.pkl')\n",
      "  model = router.route(question_features)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/2940808754.py:355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions[\"router_prediction\"] = router_predictions.values\n"
     ]
    }
   ],
   "source": [
    "# Extract these features from your data:\n",
    "features = [\n",
    "    # Existing\n",
    "    'dataset',  # one-hot encoded\n",
    "    'prompt_chars',\n",
    "    'context_chars',\n",
    "    \n",
    "    # Derive new ones\n",
    "    'question_length',  # chars in question\n",
    "    'context_to_prompt_ratio',  # context_chars / prompt_chars\n",
    "    'has_numbers',  # boolean: question contains digits\n",
    "    'question_marks_count',  # complexity indicator\n",
    "    'avg_word_length',  # vocabulary complexity\n",
    "]\n",
    "\n",
    "# Target: best_model for each question (based on which got highest EM or cosine)\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data loading / cleaning\n",
    "# -----------------------------\n",
    "def load_and_normalize(results_glob: str) -> pd.DataFrame:\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    for c in [\"type\", \"level\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"unknown\"\n",
    "        df[c] = df[c].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    # required metrics\n",
    "    if \"exact_match\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: exact_match\")\n",
    "    df[\"exact_match\"] = pd.to_numeric(df[\"exact_match\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        df[\"answer_cosine_sim\"] = pd.to_numeric(df[\"answer_cosine_sim\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    if \"model\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: model\")\n",
    "    df[\"model\"] = df[\"model\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    if \"question_id\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: question_id\")\n",
    "\n",
    "    # Ensure numeric columns\n",
    "    for col in [\"prompt_chars\", \"context_chars\", \"inference_ms\", \"router_retrieval_ms\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature Engineering\n",
    "# -----------------------------\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add derived features for ML model\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Question-level features\n",
    "    if \"question\" in df.columns:\n",
    "        df[\"question_length\"] = df[\"question\"].fillna(\"\").str.len()\n",
    "        df[\"question_word_count\"] = df[\"question\"].fillna(\"\").str.split().str.len()\n",
    "        df[\"has_numbers\"] = df[\"question\"].fillna(\"\").str.contains(r'\\d', regex=True).astype(int)\n",
    "        df[\"question_marks\"] = df[\"question\"].fillna(\"\").str.count(r'\\?')\n",
    "        df[\"avg_word_length\"] = df[\"question\"].fillna(\"\").apply(\n",
    "            lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0\n",
    "        )\n",
    "    else:\n",
    "        df[\"question_length\"] = 0\n",
    "        df[\"question_word_count\"] = 0\n",
    "        df[\"has_numbers\"] = 0\n",
    "        df[\"question_marks\"] = 0\n",
    "        df[\"avg_word_length\"] = 0\n",
    "    \n",
    "    # Context features\n",
    "    if \"context_chars\" in df.columns and \"prompt_chars\" in df.columns:\n",
    "        df[\"context_to_prompt_ratio\"] = df[\"context_chars\"] / (df[\"prompt_chars\"] + 1)\n",
    "    else:\n",
    "        df[\"context_to_prompt_ratio\"] = 0\n",
    "    \n",
    "    # Ensure no infinities or NaNs\n",
    "    df = df.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create Training Dataset\n",
    "# -----------------------------\n",
    "def prepare_training_data(df: pd.DataFrame, metric: str = \"exact_match\"):\n",
    "    \"\"\"\n",
    "    For each question, determine which model performed best.\n",
    "    Returns one row per question with features and target model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Preparing training data (optimizing for {metric}) ===\")\n",
    "    \n",
    "    # Add engineered features\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # For each question, find best performing model\n",
    "    def best_model_for_question(group):\n",
    "        # Primary: exact_match, Secondary: answer_cosine_sim\n",
    "        best_idx = group[metric].idxmax()\n",
    "        if pd.isna(best_idx) or group[metric].max() == 0:\n",
    "            if \"answer_cosine_sim\" in group.columns:\n",
    "                best_idx = group[\"answer_cosine_sim\"].idxmax()\n",
    "            else:\n",
    "                best_idx = group.index[0]  # fallback to first\n",
    "        return group.loc[best_idx, \"model\"]\n",
    "    \n",
    "    # Get best model per question\n",
    "    best_models = df.groupby(\"question_id\").apply(best_model_for_question)\n",
    "    \n",
    "    # Get one row per question with all features\n",
    "    feature_cols = [\n",
    "        \"dataset\", \"type\", \"level\",\n",
    "        \"prompt_chars\", \"context_chars\", \n",
    "        \"question_length\", \"question_word_count\", \"has_numbers\",\n",
    "        \"question_marks\", \"avg_word_length\", \"context_to_prompt_ratio\"\n",
    "    ]\n",
    "    \n",
    "    # Take first occurrence of each question (features should be same across models)\n",
    "    question_features = df.drop_duplicates(subset=[\"question_id\"]).set_index(\"question_id\")[feature_cols]\n",
    "    question_features[\"target_model\"] = best_models\n",
    "    \n",
    "    # Remove any rows with missing target\n",
    "    question_features = question_features.dropna(subset=[\"target_model\"])\n",
    "    \n",
    "    print(f\"Training samples: {len(question_features)}\")\n",
    "    print(f\"Target distribution:\\n{question_features['target_model'].value_counts()}\")\n",
    "    \n",
    "    return question_features.reset_index()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train ML Router\n",
    "# -----------------------------\n",
    "def train_ml_router(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42):\n",
    "    \"\"\"Train a LightGBM classifier to predict best model\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Training ML Router ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_df = prepare_training_data(df)\n",
    "    \n",
    "    # Separate features and target\n",
    "    categorical_features = [\"dataset\", \"type\", \"level\"]\n",
    "    numeric_features = [\n",
    "        \"prompt_chars\", \"context_chars\", \n",
    "        \"question_length\", \"question_word_count\", \"has_numbers\",\n",
    "        \"question_marks\", \"avg_word_length\", \"context_to_prompt_ratio\"\n",
    "    ]\n",
    "    \n",
    "    X = train_df[categorical_features + numeric_features].copy()\n",
    "    y = train_df[\"target_model\"]\n",
    "    \n",
    "    # Encode target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=False)\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    # Train LightGBM\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        \"feature\": X_encoded.columns,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Return model, encoder, and feature columns for inference\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"feature_columns\": X_encoded.columns.tolist(),\n",
    "        \"categorical_features\": categorical_features,\n",
    "        \"numeric_features\": numeric_features,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Router Class for Inference\n",
    "# -----------------------------\n",
    "class MLRouter:\n",
    "    def __init__(self, router_dict):\n",
    "        self.model = router_dict[\"model\"]\n",
    "        self.label_encoder = router_dict[\"label_encoder\"]\n",
    "        self.feature_columns = router_dict[\"feature_columns\"]\n",
    "        self.categorical_features = router_dict[\"categorical_features\"]\n",
    "        self.numeric_features = router_dict[\"numeric_features\"]\n",
    "        self.default_model = router_dict[\"label_encoder\"].classes_[0]  # fallback\n",
    "    \n",
    "    def route(self, question_features: dict) -> str:\n",
    "        \"\"\"\n",
    "        Route a single question to the best model.\n",
    "        \n",
    "        Args:\n",
    "            question_features: dict with keys like 'dataset', 'question', 'prompt_chars', etc.\n",
    "        \n",
    "        Returns:\n",
    "            model name (str)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Engineer features\n",
    "            features = {}\n",
    "            \n",
    "            # Extract or compute features\n",
    "            for feat in self.categorical_features:\n",
    "                features[feat] = question_features.get(feat, \"unknown\")\n",
    "            \n",
    "            for feat in self.numeric_features:\n",
    "                if feat in question_features:\n",
    "                    features[feat] = question_features[feat]\n",
    "                elif feat == \"question_length\" and \"question\" in question_features:\n",
    "                    features[feat] = len(question_features[\"question\"])\n",
    "                elif feat == \"question_word_count\" and \"question\" in question_features:\n",
    "                    features[feat] = len(question_features[\"question\"].split())\n",
    "                elif feat == \"has_numbers\" and \"question\" in question_features:\n",
    "                    features[feat] = int(bool(pd.Series([question_features[\"question\"]]).str.contains(r'\\d', regex=True).iloc[0]))\n",
    "                elif feat == \"question_marks\" and \"question\" in question_features:\n",
    "                    features[feat] = question_features[\"question\"].count('?')\n",
    "                elif feat == \"avg_word_length\" and \"question\" in question_features:\n",
    "                    words = question_features[\"question\"].split()\n",
    "                    features[feat] = np.mean([len(w) for w in words]) if words else 0\n",
    "                elif feat == \"context_to_prompt_ratio\":\n",
    "                    features[feat] = question_features.get(\"context_chars\", 0) / (question_features.get(\"prompt_chars\", 1) + 1)\n",
    "                else:\n",
    "                    features[feat] = 0\n",
    "            \n",
    "            # Create dataframe\n",
    "            df = pd.DataFrame([features])\n",
    "            \n",
    "            # One-hot encode\n",
    "            df_encoded = pd.get_dummies(df, columns=self.categorical_features, drop_first=False)\n",
    "            \n",
    "            # Ensure all training features are present\n",
    "            for col in self.feature_columns:\n",
    "                if col not in df_encoded.columns:\n",
    "                    df_encoded[col] = 0\n",
    "            \n",
    "            # Reorder to match training\n",
    "            df_encoded = df_encoded[self.feature_columns]\n",
    "            \n",
    "            # Predict\n",
    "            pred_idx = self.model.predict(df_encoded)[0]\n",
    "            model_name = self.label_encoder.inverse_transform([pred_idx])[0]\n",
    "            \n",
    "            return model_name\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Router error: {e}, falling back to default\")\n",
    "            return self.default_model\n",
    "    \n",
    "    def route_batch(self, questions_df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Route multiple questions at once\"\"\"\n",
    "        # Engineer features\n",
    "        questions_df = engineer_features(questions_df)\n",
    "        \n",
    "        # Extract features\n",
    "        X = questions_df[self.categorical_features + self.numeric_features].copy()\n",
    "        X_encoded = pd.get_dummies(X, columns=self.categorical_features, drop_first=False)\n",
    "        \n",
    "        # Ensure all training features present\n",
    "        for col in self.feature_columns:\n",
    "            if col not in X_encoded.columns:\n",
    "                X_encoded[col] = 0\n",
    "        X_encoded = X_encoded[self.feature_columns]\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(X_encoded)\n",
    "        model_names = self.label_encoder.inverse_transform(predictions)\n",
    "        \n",
    "        return pd.Series(model_names, index=questions_df.index)\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save router to disk\"\"\"\n",
    "        router_dict = {\n",
    "            \"model\": self.model,\n",
    "            \"label_encoder\": self.label_encoder,\n",
    "            \"feature_columns\": self.feature_columns,\n",
    "            \"categorical_features\": self.categorical_features,\n",
    "            \"numeric_features\": self.numeric_features\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(router_dict, f)\n",
    "        print(f\"Router saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        \"\"\"Load router from disk\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            router_dict = pickle.load(f)\n",
    "        return cls(router_dict)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluate Router Performance\n",
    "# -----------------------------\n",
    "def evaluate_router_performance(df: pd.DataFrame, router: MLRouter):\n",
    "    \"\"\"\n",
    "    Compare router performance vs baseline strategies\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Evaluating Router Performance ===\")\n",
    "    \n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # Get unique questions\n",
    "    questions = df.drop_duplicates(subset=[\"question_id\"])\n",
    "    \n",
    "    # Router predictions\n",
    "    router_predictions = router.route_batch(questions)\n",
    "    questions[\"router_prediction\"] = router_predictions.values\n",
    "    \n",
    "    # Merge predictions back to full df\n",
    "    df = df.merge(\n",
    "        questions[[\"question_id\", \"router_prediction\"]], \n",
    "        on=\"question_id\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for different strategies\n",
    "    strategies = {\n",
    "        \"ML Router\": \"router_prediction\",\n",
    "        \"Always Qwen2.5\": lambda x: \"qwen2_5\",\n",
    "        \"Dataset Best\": None  # will compute separately\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, strategy in strategies.items():\n",
    "        if strategy_name == \"ML Router\":\n",
    "            # Filter to only rows where model matches router prediction\n",
    "            selected = df[df[\"model\"] == df[strategy]]\n",
    "            results[strategy_name] = {\n",
    "                \"mean_exact_match\": selected[\"exact_match\"].mean(),\n",
    "                \"mean_cosine\": selected[\"answer_cosine_sim\"].mean() if \"answer_cosine_sim\" in selected.columns else 0\n",
    "            }\n",
    "        \n",
    "        elif strategy_name == \"Always Qwen2.5\":\n",
    "            selected = df[df[\"model\"] == \"qwen2_5\"]\n",
    "            results[strategy_name] = {\n",
    "                \"mean_exact_match\": selected[\"exact_match\"].mean(),\n",
    "                \"mean_cosine\": selected[\"answer_cosine_sim\"].mean() if \"answer_cosine_sim\" in selected.columns else 0\n",
    "            }\n",
    "        \n",
    "        elif strategy_name == \"Dataset Best\":\n",
    "            # Use best model per dataset (from leaderboard)\n",
    "            dataset_best = df.groupby([\"dataset\", \"model\"])[\"exact_match\"].mean().reset_index()\n",
    "            dataset_best = dataset_best.sort_values([\"dataset\", \"exact_match\"], ascending=[True, False])\n",
    "            dataset_best = dataset_best.groupby(\"dataset\").first().reset_index()\n",
    "            dataset_map = dict(zip(dataset_best[\"dataset\"], dataset_best[\"model\"]))\n",
    "            \n",
    "            df[\"dataset_best_model\"] = df[\"dataset\"].map(dataset_map)\n",
    "            selected = df[df[\"model\"] == df[\"dataset_best_model\"]]\n",
    "            results[strategy_name] = {\n",
    "                \"mean_exact_match\": selected[\"exact_match\"].mean(),\n",
    "                \"mean_cosine\": selected[\"answer_cosine_sim\"].mean() if \"answer_cosine_sim\" in selected.columns else 0\n",
    "            }\n",
    "    \n",
    "    # Print comparison\n",
    "    comparison = pd.DataFrame(results).T\n",
    "    print(\"\\nStrategy Comparison:\")\n",
    "    print(comparison.to_string())\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    df = load_and_normalize(results_glob)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique questions: {df['question_id'].nunique()}\")\n",
    "    print(f\"Datasets: {df['dataset'].nunique()} - {sorted(df['dataset'].unique())}\")\n",
    "    print(f\"Models: {sorted(df['model'].unique())}\")\n",
    "\n",
    "    # Train ML Router\n",
    "    router_dict = train_ml_router(df, test_size=0.2, random_state=42)\n",
    "    router = MLRouter(router_dict)\n",
    "    \n",
    "    # Save router\n",
    "    router.save(\"ml_router.pkl\")\n",
    "    \n",
    "    # Evaluate on full dataset\n",
    "    evaluate_router_performance(df, router)\n",
    "    \n",
    "    # Example inference\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE INFERENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    example_question = {\n",
    "        \"dataset\": \"hotpotqa\",\n",
    "        \"question\": \"Who was the first president of the United States?\",\n",
    "        \"prompt_chars\": 150,\n",
    "        \"context_chars\": 2500,\n",
    "        \"type\": \"factual\",\n",
    "        \"level\": \"easy\"\n",
    "    }\n",
    "    \n",
    "    predicted_model = router.route(example_question)\n",
    "    print(f\"\\nQuestion: {example_question['question']}\")\n",
    "    print(f\"Dataset: {example_question['dataset']}\")\n",
    "    print(f\"Predicted Model: {predicted_model}\")\n",
    "    \n",
    "    # Show how to load router later\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"To load router later:\")\n",
    "    print(\"  router = MLRouter.load('ml_router.pkl')\")\n",
    "    print(\"  model = router.route(question_features)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6c6efa8-95ed-470a-b2c7-ce95c4c6973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total rows: 45000\n",
      "Unique questions: 15000\n",
      "Datasets: 3 - ['hotpotqa', 'narrativeqa', 'pubmedqa']\n",
      "Models: ['llama3', 'mistral', 'qwen2_5']\n",
      "\n",
      "=== Training ML Router ===\n",
      "\n",
      "=== Preparing training data (optimizing for exact_match) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/2940808754.py:118: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  best_models = df.groupby(\"question_id\").apply(best_model_for_question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 15000\n",
      "Target distribution:\n",
      "target_model\n",
      "llama3     7729\n",
      "mistral    5077\n",
      "qwen2_5    2194\n",
      "Name: count, dtype: int64\n",
      "Train size: 12000, Test size: 3000\n",
      "\n",
      "Test Accuracy: 0.5333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      llama3       0.56      0.73      0.64      1546\n",
      "     mistral       0.47      0.46      0.47      1015\n",
      "     qwen2_5       0.40      0.00      0.01       439\n",
      "\n",
      "    accuracy                           0.53      3000\n",
      "   macro avg       0.48      0.40      0.37      3000\n",
      "weighted avg       0.51      0.53      0.49      3000\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                feature  importance\n",
      "          context_chars         807\n",
      "context_to_prompt_ratio         792\n",
      "        avg_word_length         652\n",
      "           prompt_chars         597\n",
      "    question_word_count         270\n",
      "        question_length         109\n",
      "       dataset_hotpotqa          70\n",
      "            type_bridge          49\n",
      "            has_numbers          48\n",
      "             level_easy          36\n",
      "Router saved to ml_router.pkl\n",
      "\n",
      "=== Evaluating Router Performance ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/2940808754.py:355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions[\"router_prediction\"] = router_predictions.values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy Comparison:\n",
      "                mean_exact_match  mean_cosine\n",
      "ML Router               0.150600     0.399978\n",
      "Always Qwen2.5          0.154667     0.339762\n",
      "Dataset Best            0.154667     0.328281\n",
      "\n",
      "============================================================\n",
      "DIVERSE ROUTING EXAMPLES\n",
      "============================================================\n",
      "\n",
      "Found 3 unique models: ['llama3', 'mistral', 'qwen2_5']\n",
      "\n",
      "============================================================\n",
      "EXAMPLES WHERE ROUTER CHOSE: LLAMA3\n",
      "============================================================\n",
      "\n",
      "Example 2:\n",
      "  Question ID: 16622445::q8833\n",
      "  Dataset: pubmedqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: Kazakhstan: a folate-deficient area?\n",
      "  Question Length: 36 chars, 4 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 36\n",
      "  Context Chars: 885\n",
      "  Context/Prompt Ratio: 23.92\n",
      "  Router Chose: llama3\n",
      "  Actual Best: llama3\n",
      "  Correct: True\n",
      "\n",
      "Example 3:\n",
      "  Question ID: 02476285f0673b06c7754deed4517e54c50c42b5::q369\n",
      "  Dataset: narrativeqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: What do the rats plan to do with Tom?\n",
      "  Question Length: 37 chars, 9 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 37\n",
      "  Context Chars: 9209\n",
      "  Context/Prompt Ratio: 242.34\n",
      "  Router Chose: llama3\n",
      "  Actual Best: llama3\n",
      "  Correct: True\n",
      "\n",
      "Example 1:\n",
      "  Question ID: 5a7cc87755429907fabef033\n",
      "  Dataset: hotpotqa\n",
      "  Type/Level: bridge / easy\n",
      "  Question: Radio Bam (or Bam Radio) was a Sirius Radio Station that aired on Mondays at 7pm, it had frequent guest appearances from  Bam's parents April Margera ...\n",
      "  Question Length: 331 chars, 55 words\n",
      "  Has Numbers: Yes\n",
      "  Prompt Chars: 331\n",
      "  Context Chars: 5099\n",
      "  Context/Prompt Ratio: 15.36\n",
      "  Router Chose: llama3\n",
      "  Actual Best: llama3\n",
      "  Correct: True\n",
      "\n",
      "============================================================\n",
      "EXAMPLES WHERE ROUTER CHOSE: MISTRAL\n",
      "============================================================\n",
      "\n",
      "Example 2:\n",
      "  Question ID: 23351626::q8108\n",
      "  Dataset: pubmedqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: Does compliance with nutrition guidelines lead to healthy aging?\n",
      "  Question Length: 64 chars, 9 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 64\n",
      "  Context Chars: 2141\n",
      "  Context/Prompt Ratio: 32.94\n",
      "  Router Chose: mistral\n",
      "  Actual Best: mistral\n",
      "  Correct: True\n",
      "\n",
      "Example 3:\n",
      "  Question ID: 09355a61a4d84807f9533f31d3263809cc486b6b::q1153\n",
      "  Dataset: narrativeqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: What did the news report say would kill the monsters?\n",
      "  Question Length: 53 chars, 10 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 53\n",
      "  Context Chars: 4953\n",
      "  Context/Prompt Ratio: 91.72\n",
      "  Router Chose: mistral\n",
      "  Actual Best: mistral\n",
      "  Correct: True\n",
      "\n",
      "Example 1:\n",
      "  Question ID: 23660564::q2412\n",
      "  Dataset: pubmedqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: Do breaks in gastroenterology fellow endoscopy training result in a decrement in competency in colonoscopy?\n",
      "  Question Length: 107 chars, 15 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 107\n",
      "  Context Chars: 1564\n",
      "  Context/Prompt Ratio: 14.48\n",
      "  Router Chose: mistral\n",
      "  Actual Best: mistral\n",
      "  Correct: True\n",
      "\n",
      "============================================================\n",
      "EXAMPLES WHERE ROUTER CHOSE: QWEN2_5\n",
      "============================================================\n",
      "\n",
      "Example 2:\n",
      "  Question ID: 5ac3b879554299204fd21e17\n",
      "  Dataset: hotpotqa\n",
      "  Type/Level: bridge / easy\n",
      "  Question: The 2016 Miami Dolphins season was the franchise's 47th season in the National Football League, the 51st overall and the first under head coach Adam G...\n",
      "  Question Length: 542 chars, 94 words\n",
      "  Has Numbers: Yes\n",
      "  Prompt Chars: 542\n",
      "  Context Chars: 7189\n",
      "  Context/Prompt Ratio: 13.24\n",
      "  Router Chose: qwen2_5\n",
      "  Actual Best: qwen2_5\n",
      "  Correct: True\n",
      "\n",
      "Example 3:\n",
      "  Question ID: 0bc7352d6a0e678c0d8acc57c0c1cc3466fe9ef7::q1470\n",
      "  Dataset: narrativeqa\n",
      "  Type/Level: unknown / unknown\n",
      "  Question: Who is Keisha?\n",
      "  Question Length: 14 chars, 3 words\n",
      "  Has Numbers: No\n",
      "  Prompt Chars: 14\n",
      "  Context Chars: 9777\n",
      "  Context/Prompt Ratio: 651.80\n",
      "  Router Chose: qwen2_5\n",
      "  Actual Best: qwen2_5\n",
      "  Correct: True\n",
      "\n",
      "Example 1:\n",
      "  Question ID: 5a7ced03554299683c1c63e2\n",
      "  Dataset: hotpotqa\n",
      "  Type/Level: bridge / easy\n",
      "  Question: Edsger Wybe Dijkstra (] ; 11 May 1930 – 6 August 2002) was a Dutch computer scientist and an early pioneer in many research areas of computing science...\n",
      "  Question Length: 407 chars, 67 words\n",
      "  Has Numbers: Yes\n",
      "  Prompt Chars: 407\n",
      "  Context Chars: 7070\n",
      "  Context/Prompt Ratio: 17.33\n",
      "  Router Chose: qwen2_5\n",
      "  Actual Best: qwen2_5\n",
      "  Correct: True\n",
      "\n",
      "============================================================\n",
      "ROUTING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Routing Statistics:\n",
      "                   count  accuracy  question_length  context_chars  context_to_prompt_ratio\n",
      "router_prediction                                                                          \n",
      "llama3              9931  0.595308        79.981674    6090.886920               106.414436\n",
      "mistral             5040  0.526190        89.659722    2998.969444                45.329088\n",
      "qwen2_5               29  0.758621       213.482759    8554.655172               296.322485\n",
      "\n",
      "============================================================\n",
      "FEATURE PATTERNS BY MODEL\n",
      "============================================================\n",
      "\n",
      "LLAMA3 - Most common characteristics:\n",
      "  Dataset distribution: {'hotpotqa': 4986, 'narrativeqa': 3625, 'pubmedqa': 1320}\n",
      "  Avg question length: 80 chars\n",
      "  Avg context size: 6091 chars\n",
      "  Questions with numbers: 15.6%\n",
      "\n",
      "MISTRAL - Most common characteristics:\n",
      "  Dataset distribution: {'pubmedqa': 3680, 'narrativeqa': 1358, 'hotpotqa': 2}\n",
      "  Avg question length: 90 chars\n",
      "  Avg context size: 2999 chars\n",
      "  Questions with numbers: 6.3%\n",
      "\n",
      "QWEN2_5 - Most common characteristics:\n",
      "  Dataset distribution: {'narrativeqa': 17, 'hotpotqa': 12}\n",
      "  Avg question length: 213 chars\n",
      "  Avg context size: 8555 chars\n",
      "  Questions with numbers: 34.5%\n",
      "\n",
      "============================================================\n",
      "EXAMPLE INFERENCE\n",
      "============================================================\n",
      "\n",
      "Question: Who was the first president of the United States?\n",
      "Dataset: hotpotqa\n",
      "Predicted Model: llama3\n",
      "\n",
      "============================================================\n",
      "To load router later:\n",
      "  router = MLRouter.load('ml_router.pkl')\n",
      "  model = router.route(question_features)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Add this function after the evaluate_router_performance function\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Find Diverse Routing Examples\n",
    "# -----------------------------\n",
    "def find_diverse_examples(df: pd.DataFrame, router: MLRouter, n_per_model: int = 3):\n",
    "    \"\"\"\n",
    "    Find real examples from the dataset where router predicts different models.\n",
    "    Shows the question characteristics and why each model was chosen.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIVERSE ROUTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Engineer features\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # Get unique questions with all features\n",
    "    questions = df.drop_duplicates(subset=[\"question_id\"]).copy()\n",
    "    \n",
    "    # Get router predictions\n",
    "    router_predictions = router.route_batch(questions)\n",
    "    questions[\"router_prediction\"] = router_predictions.values\n",
    "    \n",
    "    # Get the actual best model for each question (ground truth)\n",
    "    def get_actual_best(question_id):\n",
    "        q_data = df[df[\"question_id\"] == question_id]\n",
    "        best_idx = q_data[\"exact_match\"].idxmax()\n",
    "        if pd.isna(best_idx) or q_data[\"exact_match\"].max() == 0:\n",
    "            best_idx = q_data[\"answer_cosine_sim\"].idxmax()\n",
    "        return q_data.loc[best_idx, \"model\"]\n",
    "    \n",
    "    questions[\"actual_best\"] = questions[\"question_id\"].apply(get_actual_best)\n",
    "    questions[\"correct_prediction\"] = questions[\"router_prediction\"] == questions[\"actual_best\"]\n",
    "    \n",
    "    # Get all unique models\n",
    "    all_models = sorted(questions[\"router_prediction\"].unique())\n",
    "    \n",
    "    print(f\"\\nFound {len(all_models)} unique models: {all_models}\\n\")\n",
    "    \n",
    "    examples_collection = []\n",
    "    \n",
    "    for model in all_models:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"EXAMPLES WHERE ROUTER CHOSE: {model.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Get questions where router predicted this model\n",
    "        model_questions = questions[questions[\"router_prediction\"] == model]\n",
    "        \n",
    "        if len(model_questions) == 0:\n",
    "            print(f\"No examples found for {model}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Prioritize correct predictions\n",
    "        correct_predictions = model_questions[model_questions[\"correct_prediction\"] == True]\n",
    "        \n",
    "        if len(correct_predictions) >= n_per_model:\n",
    "            sample = correct_predictions.sample(n=min(n_per_model, len(correct_predictions)), random_state=42)\n",
    "        else:\n",
    "            # If not enough correct, take what we have + some incorrect\n",
    "            sample = model_questions.sample(n=min(n_per_model, len(model_questions)), random_state=42)\n",
    "        \n",
    "        for idx, row in sample.iterrows():\n",
    "            examples_collection.append({\n",
    "                \"model\": model,\n",
    "                \"question_id\": row[\"question_id\"],\n",
    "                \"dataset\": row[\"dataset\"],\n",
    "                \"question\": row[\"question\"][:100] + \"...\" if len(str(row[\"question\"])) > 100 else row[\"question\"],\n",
    "                \"question_length\": row[\"question_length\"],\n",
    "                \"question_word_count\": row[\"question_word_count\"],\n",
    "                \"has_numbers\": row[\"has_numbers\"],\n",
    "                \"prompt_chars\": row[\"prompt_chars\"],\n",
    "                \"context_chars\": row[\"context_chars\"],\n",
    "                \"context_to_prompt_ratio\": row[\"context_to_prompt_ratio\"],\n",
    "                \"type\": row[\"type\"],\n",
    "                \"level\": row[\"level\"],\n",
    "                \"actual_best\": row[\"actual_best\"],\n",
    "                \"correct\": \"✓\" if row[\"correct_prediction\"] else \"✗\"\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nExample {len(examples_collection) % n_per_model + 1}:\")\n",
    "            print(f\"  Question ID: {row['question_id']}\")\n",
    "            print(f\"  Dataset: {row['dataset']}\")\n",
    "            print(f\"  Type/Level: {row['type']} / {row['level']}\")\n",
    "            print(f\"  Question: {row['question'][:150]}{'...' if len(str(row['question'])) > 150 else ''}\")\n",
    "            print(f\"  Question Length: {row['question_length']} chars, {row['question_word_count']} words\")\n",
    "            print(f\"  Has Numbers: {'Yes' if row['has_numbers'] else 'No'}\")\n",
    "            print(f\"  Prompt Chars: {row['prompt_chars']}\")\n",
    "            print(f\"  Context Chars: {row['context_chars']}\")\n",
    "            print(f\"  Context/Prompt Ratio: {row['context_to_prompt_ratio']:.2f}\")\n",
    "            print(f\"  Router Chose: {model}\")\n",
    "            print(f\"  Actual Best: {row['actual_best']}\")\n",
    "            print(f\"  Correct: {row['correct_prediction']}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ROUTING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary = questions.groupby(\"router_prediction\").agg({\n",
    "        \"question_id\": \"count\",\n",
    "        \"correct_prediction\": \"mean\",\n",
    "        \"question_length\": \"mean\",\n",
    "        \"context_chars\": \"mean\",\n",
    "        \"context_to_prompt_ratio\": \"mean\"\n",
    "    }).rename(columns={\n",
    "        \"question_id\": \"count\",\n",
    "        \"correct_prediction\": \"accuracy\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nRouting Statistics:\")\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # Feature characteristics by predicted model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE PATTERNS BY MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model in all_models:\n",
    "        model_data = questions[questions[\"router_prediction\"] == model]\n",
    "        print(f\"\\n{model.upper()} - Most common characteristics:\")\n",
    "        print(f\"  Dataset distribution: {model_data['dataset'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg question length: {model_data['question_length'].mean():.0f} chars\")\n",
    "        print(f\"  Avg context size: {model_data['context_chars'].mean():.0f} chars\")\n",
    "        print(f\"  Questions with numbers: {model_data['has_numbers'].mean()*100:.1f}%\")\n",
    "    \n",
    "    return pd.DataFrame(examples_collection)\n",
    "\n",
    "\n",
    "# Update the main() function to include this:\n",
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    df = load_and_normalize(results_glob)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique questions: {df['question_id'].nunique()}\")\n",
    "    print(f\"Datasets: {df['dataset'].nunique()} - {sorted(df['dataset'].unique())}\")\n",
    "    print(f\"Models: {sorted(df['model'].unique())}\")\n",
    "\n",
    "    # Train ML Router\n",
    "    router_dict = train_ml_router(df, test_size=0.2, random_state=42)\n",
    "    router = MLRouter(router_dict)\n",
    "    \n",
    "    # Save router\n",
    "    router.save(\"ml_router.pkl\")\n",
    "    \n",
    "    # Evaluate on full dataset\n",
    "    evaluate_router_performance(df, router)\n",
    "    \n",
    "    # Find diverse examples - NEW!\n",
    "    examples_df = find_diverse_examples(df, router, n_per_model=3)\n",
    "    \n",
    "    # Example inference\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE INFERENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    example_question = {\n",
    "        \"dataset\": \"hotpotqa\",\n",
    "        \"question\": \"Who was the first president of the United States?\",\n",
    "        \"prompt_chars\": 150,\n",
    "        \"context_chars\": 2500,\n",
    "        \"type\": \"factual\",\n",
    "        \"level\": \"easy\"\n",
    "    }\n",
    "    \n",
    "    predicted_model = router.route(example_question)\n",
    "    print(f\"\\nQuestion: {example_question['question']}\")\n",
    "    print(f\"Dataset: {example_question['dataset']}\")\n",
    "    print(f\"Predicted Model: {predicted_model}\")\n",
    "    \n",
    "    # Show how to load router later\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"To load router later:\")\n",
    "    print(\"  router = MLRouter.load('ml_router.pkl')\")\n",
    "    print(\"  model = router.route(question_features)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8aed3e-3043-4655-9f99-9f84611ee7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93aa61-cd79-423d-bd40-7e0c2d567b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e662df1-eae7-4993-8d78-c272aabb5701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8af020-8f07-43f6-b99e-576becf0ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4ec89-fab6-436c-8c79-31817fa622e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c75f3f8-efc9-40f8-b019-3bef8983c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 45000\n",
      "unique questions: 15000\n",
      "datasets: 3\n",
      "models: ['llama3', 'mistral', 'qwen2_5']\n",
      "\n",
      "Default (global best by mean EM): qwen2_5\n",
      "\n",
      "=== Best model per dataset ===\n",
      "    dataset best_model  mean_exact_match  mean_cosine\n",
      "   hotpotqa    qwen2_5             0.464     0.691563\n",
      "narrativeqa    mistral             0.000     0.336478\n",
      "   pubmedqa    mistral             0.000     0.238067\n",
      "\n",
      "=== Full leaderboard (top 3 per dataset) ===\n",
      "    dataset   model  mean_exact_match  mean_cosine\n",
      "   hotpotqa qwen2_5            0.4640     0.691563\n",
      "   hotpotqa  llama3            0.4504     0.697455\n",
      "   hotpotqa mistral            0.4348     0.703102\n",
      "narrativeqa  llama3            0.0000     0.283110\n",
      "narrativeqa mistral            0.0000     0.336478\n",
      "narrativeqa qwen2_5            0.0000     0.296356\n",
      "   pubmedqa  llama3            0.0000     0.010169\n",
      "   pubmedqa mistral            0.0000     0.238067\n",
      "   pubmedqa qwen2_5            0.0000     0.031367\n",
      "\n",
      "Example routing:\n",
      "  hotpotqa        -> qwen2_5\n",
      "  narrativeqa     -> mistral\n",
      "  pubmedqa        -> mistral\n",
      "\n",
      "✓ Saved router to: ./router_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data loading / cleaning\n",
    "# -----------------------------\n",
    "def load_and_normalize(results_glob: str) -> pd.DataFrame:\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    for c in [\"type\", \"level\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"unknown\"\n",
    "        df[c] = df[c].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    # required metrics\n",
    "    if \"exact_match\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: exact_match\")\n",
    "    df[\"exact_match\"] = pd.to_numeric(df[\"exact_match\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "    \n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        df[\"answer_cosine_sim\"] = pd.to_numeric(df[\"answer_cosine_sim\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "    else:\n",
    "        df[\"answer_cosine_sim\"] = 0.0\n",
    "\n",
    "    if \"model\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: model\")\n",
    "    df[\"model\"] = df[\"model\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    if \"question_id\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: question_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Best model per dataset\n",
    "# -----------------------------\n",
    "def dataset_leaderboard(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      winners_df: one row per dataset with best model\n",
    "      leaderboard_df: all dataset x model rows with metrics\n",
    "    \"\"\"\n",
    "    agg = {\"exact_match\": \"mean\"}\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        agg[\"answer_cosine_sim\"] = \"mean\"\n",
    "\n",
    "    leaderboard = (\n",
    "        df.groupby([\"dataset\", \"model\"], as_index=False)\n",
    "          .agg(agg)\n",
    "          .rename(columns={\n",
    "              \"exact_match\": \"mean_exact_match\",\n",
    "              \"answer_cosine_sim\": \"mean_cosine\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    sort_cols = [\"dataset\", \"mean_exact_match\"]\n",
    "    ascending = [True, False]\n",
    "    if \"mean_cosine\" in leaderboard.columns:\n",
    "        sort_cols.append(\"mean_cosine\")\n",
    "        ascending.append(False)\n",
    "    sort_cols.append(\"model\")\n",
    "    ascending.append(True)\n",
    "\n",
    "    winners = (\n",
    "        leaderboard.sort_values(sort_cols, ascending=ascending)\n",
    "                   .groupby(\"dataset\", as_index=False)\n",
    "                   .first()\n",
    "                   .rename(columns={\"model\": \"best_model\"})\n",
    "    )\n",
    "\n",
    "    return winners, leaderboard\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Optional: Best per dataset+type+level\n",
    "# -----------------------------\n",
    "def bucket_leaderboard(df: pd.DataFrame, bucket_cols=(\"dataset\", \"type\", \"level\")):\n",
    "    agg = {\"exact_match\": \"mean\"}\n",
    "    if \"answer_cosine_sim\" in df.columns:\n",
    "        agg[\"answer_cosine_sim\"] = \"mean\"\n",
    "\n",
    "    leaderboard = (\n",
    "        df.groupby(list(bucket_cols) + [\"model\"], as_index=False)\n",
    "          .agg(agg)\n",
    "          .rename(columns={\n",
    "              \"exact_match\": \"mean_exact_match\",\n",
    "              \"answer_cosine_sim\": \"mean_cosine\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    sort_cols = list(bucket_cols) + [\"mean_exact_match\"]\n",
    "    ascending = [True] * len(bucket_cols) + [False]\n",
    "    if \"mean_cosine\" in leaderboard.columns:\n",
    "        sort_cols.append(\"mean_cosine\")\n",
    "        ascending.append(False)\n",
    "    sort_cols.append(\"model\")\n",
    "    ascending.append(True)\n",
    "\n",
    "    winners = (\n",
    "        leaderboard.sort_values(sort_cols, ascending=ascending)\n",
    "                   .groupby(list(bucket_cols), as_index=False)\n",
    "                   .first()\n",
    "                   .rename(columns={\"model\": \"best_model\"})\n",
    "    )\n",
    "\n",
    "    return winners, leaderboard\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Switcher functions\n",
    "# -----------------------------\n",
    "def make_dataset_switcher(winners_df: pd.DataFrame, default_model: str):\n",
    "    mapping = dict(zip(winners_df[\"dataset\"].tolist(), winners_df[\"best_model\"].tolist()))\n",
    "\n",
    "    def choose(dataset: str) -> str:\n",
    "        d = (dataset or \"unknown\").strip().lower()\n",
    "        return mapping.get(d, default_model)\n",
    "\n",
    "    return choose, mapping\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    out_path = os.getenv(\"ROUTER_MODEL_PATH\", \"./router_model.joblib\")\n",
    "    \n",
    "    df = load_and_normalize(results_glob)\n",
    "\n",
    "    print(\"rows:\", len(df))\n",
    "    print(\"unique questions:\", df[\"question_id\"].nunique())\n",
    "    print(\"datasets:\", df[\"dataset\"].nunique())\n",
    "    print(\"models:\", sorted(df[\"model\"].unique()))\n",
    "\n",
    "    # choose a sane default model = best overall mean exact_match\n",
    "    default_model = (\n",
    "        df.groupby(\"model\")[\"exact_match\"].mean().sort_values(ascending=False).index[0]\n",
    "    )\n",
    "    print(\"\\nDefault (global best by mean EM):\", default_model)\n",
    "\n",
    "    # -------- dataset-only winners --------\n",
    "    winners, leaderboard = dataset_leaderboard(df)\n",
    "\n",
    "    print(\"\\n=== Best model per dataset ===\")\n",
    "    print(winners.sort_values(\"dataset\").to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== Full leaderboard (top 3 per dataset) ===\")\n",
    "    top3 = leaderboard.sort_values([\"dataset\", \"mean_exact_match\"], ascending=[True, False]) \\\n",
    "                      .groupby(\"dataset\").head(3)\n",
    "    print(top3.to_string(index=False))\n",
    "\n",
    "    choose_model, mapping = make_dataset_switcher(winners, default_model)\n",
    "\n",
    "    # Example usage:\n",
    "    print(\"\\nExample routing:\")\n",
    "    for ds in sorted(df[\"dataset\"].unique()):\n",
    "        print(f\"  {ds:15s} -> {choose_model(ds)}\")\n",
    "\n",
    "    # -------- Save router model --------\n",
    "    router_obj = {\n",
    "        \"router_type\": \"dataset\",\n",
    "        \"dataset_router\": mapping,\n",
    "        \"default_model\": default_model,\n",
    "        \"meta\": {\n",
    "            \"num_datasets\": len(mapping),\n",
    "            \"num_samples\": len(df),\n",
    "            \"num_questions\": df[\"question_id\"].nunique(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    joblib.dump(router_obj, out_path)\n",
    "    print(f\"\\n✓ Saved router to: {out_path}\")\n",
    "    \n",
    "    # -------- optional: dataset+type+level winners --------\n",
    "    # bucket_winners, bucket_lb = bucket_leaderboard(df, (\"dataset\",\"type\",\"level\"))\n",
    "    # print(\"\\n=== Best model per dataset+type+level ===\")\n",
    "    # print(bucket_winners.head(50).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab98be67-3a48-4bb1-8020-c58830bf84e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DATASET router artifact: ./data\n",
      "Dataset mapping: {'hotpotqa': 'qwen2_5', 'narrativeqa': 'mistral', 'pubmedqa': 'mistral'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# from s3_artifacts import upload_file\n",
    "\n",
    "\n",
    "def load_df(results_glob: str) -> pd.DataFrame:\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    # dataset/collection normalization\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    # model\n",
    "    if \"model\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: model\")\n",
    "    df[\"model\"] = df[\"model\"].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    # metrics\n",
    "    if \"exact_match\" not in df.columns:\n",
    "        raise SystemExit(\"Missing required column: exact_match\")\n",
    "    df[\"exact_match\"] = pd.to_numeric(df[\"exact_match\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    if \"answer_cosine_sim\" not in df.columns:\n",
    "        df[\"answer_cosine_sim\"] = 0.0\n",
    "    df[\"answer_cosine_sim\"] = pd.to_numeric(df[\"answer_cosine_sim\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def maybe_upload(out_path: str):\n",
    "    bucket = os.getenv(\"S3_BUCKET\", \"\").strip()\n",
    "    key = os.getenv(\"S3_KEY\", \"\").strip()\n",
    "    enabled = os.getenv(\"ROUTER_MODEL_S3_UPLOAD\", \"1\").strip()  # default ON\n",
    "\n",
    "    if enabled != \"1\":\n",
    "        print(\"S3 upload disabled via ROUTER_MODEL_S3_UPLOAD!=1\")\n",
    "        return\n",
    "    if not bucket or not key:\n",
    "        print(\"S3_BUCKET/S3_KEY not set; skipping upload\")\n",
    "        return\n",
    "\n",
    "    upload_file(out_path, bucket, key)\n",
    "    print(f\"Uploaded router artifact to s3://{bucket}/{key}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    out_path = os.getenv(\"ROUTER_MODEL_PATH\", \"/data/router_model/router_model.joblib\")\n",
    "\n",
    "    # weights for score (latency is NOT used)\n",
    "    em_w = float(os.getenv(\"ROUTER_EM_WEIGHT\", \"0.25\"))\n",
    "    cos_w = float(os.getenv(\"ROUTER_COS_WEIGHT\", \"1.0\"))\n",
    "\n",
    "    df = load_df(results_glob)\n",
    "\n",
    "    # dataset x model leaderboard\n",
    "    leaderboard = (\n",
    "        df.groupby([\"dataset\", \"model\"], as_index=False)\n",
    "          .agg(\n",
    "              mean_em=(\"exact_match\", \"mean\"),\n",
    "              mean_cos=(\"answer_cosine_sim\", \"mean\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # ✅ IMPORTANT: sort by *metrics* (not just model), then take first per dataset\n",
    "    winners = (\n",
    "        leaderboard.sort_values(\n",
    "            [\"dataset\", \"mean_em\", \"mean_cos\", \"model\"],\n",
    "            ascending=[True, False, False, True],\n",
    "        )\n",
    "        .groupby(\"dataset\", as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    mapping = dict(zip(winners[\"dataset\"].tolist(), winners[\"model\"].tolist()))\n",
    "\n",
    "    obj = {\n",
    "        \"router_type\": \"dataset\",\n",
    "        \"dataset_router\": mapping,\n",
    "        \"meta\": {\n",
    "            \"em_weight\": em_w,\n",
    "            \"cos_weight\": cos_w,\n",
    "            \"uses_latency\": False,\n",
    "            \"rows\": int(len(df)),\n",
    "            \"datasets\": int(df[\"dataset\"].nunique()),\n",
    "            \"models\": sorted(df[\"model\"].unique().tolist()),\n",
    "        },\n",
    "        # optional debug info\n",
    "        \"dataset_leaderboard\": winners[\n",
    "            [\"dataset\", \"model\", \"mean_em\", \"mean_cos\"]\n",
    "        ].to_dict(\"records\"),\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    joblib.dump(obj, out_path)\n",
    "    print(f\"Saved DATASET router artifact: {out_path}\")\n",
    "    print(\"Dataset mapping:\", mapping)\n",
    "\n",
    "    maybe_upload(out_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4a7a4bde-180b-4f56-97ef-22925535fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Raw data summary (before building labels)\n",
      "================================================================================\n",
      "CSV files: 3\n",
      "Rows (model-runs): 45,000\n",
      "Unique questions: 15,000\n",
      "Unique models: 3\n",
      "\n",
      "Top datasets (by runs):\n",
      "dataset\n",
      "hotpotqa       15000\n",
      "narrativeqa    15000\n",
      "pubmedqa       15000\n",
      "\n",
      "Feature stats (runs):\n",
      "       prompt_chars  context_chars\n",
      "count  45000.000000   45000.000000\n",
      "mean      83.491600    5056.765933\n",
      "std       48.900883    3101.038751\n",
      "min       11.000000       0.000000\n",
      "25%       51.000000    1693.000000\n",
      "50%       75.000000    5525.000000\n",
      "75%      103.250000    7525.250000\n",
      "max      630.000000   12153.000000\n",
      "\n",
      "Latency columns present: {True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151320/1825085395.py:98: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y = df.groupby(key).apply(pick_best_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Target label summary (best model per question)\n",
      "================================================================================\n",
      "mistral    10094\n",
      "llama3      3254\n",
      "qwen2_5     1652\n",
      "\n",
      "================================================================================\n",
      "Test results\n",
      "================================================================================\n",
      "Accuracy:          0.4673\n",
      "Balanced accuracy: 0.4631\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      llama3       0.34      0.63      0.44       651\n",
      "     mistral       0.84      0.44      0.58      2019\n",
      "     qwen2_5       0.15      0.32      0.20       330\n",
      "\n",
      "    accuracy                           0.47      3000\n",
      "   macro avg       0.44      0.46      0.41      3000\n",
      "weighted avg       0.65      0.47      0.51      3000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "              pred:llama3  pred:mistral  pred:qwen2_5\n",
      "true:llama3           407            55           189\n",
      "true:mistral          699           888           432\n",
      "true:qwen2_5          108           115           107\n",
      "\n",
      "================================================================================\n",
      "Saved\n",
      "================================================================================\n",
      "Saved ML router artifact: /data/router.joblib\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "def pick_best_model(group: pd.DataFrame) -> str:\n",
    "    em_w = float(os.getenv(\"ROUTER_EM_WEIGHT\", \"0.25\"))\n",
    "    lat_w = float(os.getenv(\"ROUTER_LAT_WEIGHT\", \"0.0002\"))\n",
    "\n",
    "    inf_ms = group.get(\"inference_ms\", pd.Series([0.0] * len(group))).fillna(\n",
    "        group.get(\"client_elapsed_ms\", pd.Series([0.0] * len(group))).fillna(0.0)\n",
    "    )\n",
    "\n",
    "    utility = (\n",
    "        group.get(\"answer_cosine_sim\", 0.0).fillna(0.0)\n",
    "        + em_w * group.get(\"exact_match\", 0.0).fillna(0.0)\n",
    "        - lat_w * inf_ms\n",
    "    )\n",
    "    best_idx = utility.idxmax()\n",
    "    return str(group.loc[best_idx, \"model\"])\n",
    "\n",
    "\n",
    "def print_header(title: str) -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    results_glob = os.getenv(\"ROUTER_TRAIN_GLOB\", \"/data/results/**/evaluation_results.csv\")\n",
    "    out_path = os.getenv(\"ROUTER_MODEL_PATH\", \"/data/router.joblib\")\n",
    "    test_size = float(os.getenv(\"ROUTER_TEST_SIZE\", \"0.2\"))\n",
    "    seed = int(os.getenv(\"ROUTER_SEED\", \"42\"))\n",
    "\n",
    "    csvs = sorted(glob.glob(results_glob, recursive=True))\n",
    "    if not csvs:\n",
    "        raise SystemExit(f\"No CSVs found for ROUTER_TRAIN_GLOB={results_glob}\")\n",
    "\n",
    "    df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "\n",
    "    # Normalize dataset / collection\n",
    "    if \"dataset\" not in df.columns:\n",
    "        df[\"dataset\"] = \"\"\n",
    "    if \"collection\" not in df.columns:\n",
    "        df[\"collection\"] = \"unknown\"\n",
    "\n",
    "    df[\"dataset\"] = df[\"dataset\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df[\"collection\"] = df[\"collection\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = df.loc[df[\"dataset\"] == \"\", \"collection\"]\n",
    "    df.loc[df[\"dataset\"] == \"\", \"dataset\"] = \"unknown\"\n",
    "\n",
    "    # Ensure feature cols exist\n",
    "    for c in [\"prompt_chars\", \"context_chars\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "\n",
    "    required = [\"question_id\", \"model\", \"answer_cosine_sim\", \"exact_match\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise SystemExit(f\"Missing required columns in eval CSV(s): {missing}\")\n",
    "\n",
    "    key = \"question_id\"\n",
    "\n",
    "    # ---- Raw data summary ----\n",
    "    print_header(\"Raw data summary (before building labels)\")\n",
    "    print(f\"CSV files: {len(csvs)}\")\n",
    "    print(f\"Rows (model-runs): {len(df):,}\")\n",
    "    print(f\"Unique questions: {df[key].nunique():,}\")\n",
    "    print(f\"Unique models: {df['model'].nunique():,}\")\n",
    "\n",
    "    print(\"\\nTop datasets (by runs):\")\n",
    "    print(df[\"dataset\"].value_counts().head(20).to_string())\n",
    "\n",
    "    print(\"\\nFeature stats (runs):\")\n",
    "    print(df[[\"prompt_chars\", \"context_chars\"]].describe().to_string())\n",
    "\n",
    "    print(\"\\nLatency columns present:\",\n",
    "          {\"inference_ms\" in df.columns, \"client_elapsed_ms\" in df.columns})\n",
    "\n",
    "    # ---- Build target y ----\n",
    "    try:\n",
    "        y = df.groupby(key, include_groups=False).apply(pick_best_model)\n",
    "    except TypeError:\n",
    "        y = df.groupby(key).apply(pick_best_model)\n",
    "\n",
    "    first = df.groupby(key).first().copy()\n",
    "    X = first[[\"dataset\", \"prompt_chars\", \"context_chars\"]].copy()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    print_header(\"Target label summary (best model per question)\")\n",
    "    print(y.value_counts().to_string())\n",
    "\n",
    "    # ---- Train/test split ----\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "        )\n",
    "    except ValueError:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=seed\n",
    "        )\n",
    "\n",
    "    # ---- Train pipeline ----\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"dataset\"]),\n",
    "            (\"num\", \"passthrough\", [\"prompt_chars\", \"context_chars\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000, class_weight=\"balanced\")\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ---- Test evaluation ----\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    print_header(\"Test results\")\n",
    "    print(f\"Accuracy:          {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    labels = sorted(pd.unique(pd.concat([y_test, pd.Series(y_pred)])))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"true:{l}\" for l in labels],\n",
    "        columns=[f\"pred:{l}\" for l in labels],\n",
    "    )\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(cm_df.to_string())\n",
    "\n",
    "    # ---- Save artifact ----\n",
    "    obj = {\n",
    "        \"router_type\": \"ml\",\n",
    "        \"ml_model\": pipe,\n",
    "        \"meta\": {\n",
    "            \"em_weight\": float(os.getenv(\"ROUTER_EM_WEIGHT\", \"0.25\")),\n",
    "            \"lat_weight\": float(os.getenv(\"ROUTER_LAT_WEIGHT\", \"0.0002\")),\n",
    "            \"test_size\": test_size,\n",
    "            \"seed\": seed,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    joblib.dump(obj, out_path)\n",
    "\n",
    "    print_header(\"Saved\")\n",
    "    print(f\"Saved ML router artifact: {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd8fc2-1b0f-4f89-aaaa-eb9300bf263c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
